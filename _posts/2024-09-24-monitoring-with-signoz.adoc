---
layout: post
title: Monitoring with SigNoz
description: This blog post summarizes observability, briefly explains the missing pillar from a previous post and introduces the monitoring platform SigNoz.
date: 2024-09-24 17:57 +0200
last_updated: 2024-09-24 17:57 +0200
author: Christoph Kappel
tags: golang tracing opentelemetry logging prometheus logsprout alertmanager signoz showcase
categories: observability
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/monitoring_with_signoz
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/monitoring_with_signoz
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:
:source-highlighter: highlight.js

:1: https://prometheus.io/docs/alerting/latest/alertmanager/
:2: https://medium.com/@letathenasleep/alerting-the-dos-and-don-ts-for-effective-observability-139db9fb49d1
:3: https://clickhouse.com/
:4: https://docs.docker.com/compose/
:5: https://www.docker.com/
:6: https://signoz.io/docs/userguide/alerts-management/
:7: https://www.digitalocean.com/community/tutorials/elasticsearch-fluentd-and-kibana-open-source-log-search-and-visualization
:8: https://itsfoss.com/what-is-foss/
:9: https://gin-gonic.com/docs/examples/using-middleware/
:10: https://github.com/
:11: https://go.dev/
:12: https://www.elastic.co/kibana
:14: https://github.com/gliderlabs/logspout
:15: https://opentelemetry.io/docs/collector/
:16: https://opentelemetry.io/
:17: https://opentelemetry.io/docs/collector/
:18: https://pkg.go.dev/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp
:19: https://opentelemetry.io/docs/collector/configuration/
:20: https://make.powerautomate.com/
:21: https://prometheus.io/
:22: https://signoz.io/
:23: https://opentelemetry.io/docs/concepts/signals/traces/#attributes
:24: https://opentelemetry.io/docs/concepts/signals/traces/#spans
:25: https://www.crowdstrike.com/cybersecurity-101/observability/three-pillars-of-observability/
:26: https://opentelemetry.io/docs/concepts/signals/traces/#tracer
:27: https://github.com/rs/zerolog
:28: https://prometheus.io/docs/concepts/metric_types/
:29: https://grafana.com/
:30: https://www.forbes.com/sites/duenablomstrom1/2018/11/30/nobody-gets-fired-for-buying-ibm-but-they-should/

Finding good reasoning to explore different options for monitoring or better [observability] is
difficult.
Either there wasn't the singular impact on production yet, that made you lust for better monitoring
and/or it is difficult to understand the merit and invest of time.

And even when you make the decision to dive into it, it is always a good idea not to start on
production, but with a simple example.
Simple examples on the other hand rarely show the real powers, so it usually ends in heavily
contrived ones like the one I've used in my last post about
{{ site.url }}{% post_url 2022-02-18-logging-vs-tracing %}[Logging vs Tracing].

Still, [line-through]#{30}[nobody got fired for buying IBM]# ramping up monitoring, so let us - for
the course of this post - put our {7}[EFK] stack and friends aside and get started with something
shiny new in {11}[Golang].

== What is SigNoz?

If you are like me and you haven't heard the name {22}[SigNoz] before the first and foremost
questions are probably what is SigNoz and why not one of these solutions
`insert random product here`.

From a marketing perspective the key selling point for me probably and honestly was the headline
on the frontpage:

[quote,https://signoz.io]
____
OpenTelemetry-Native Logs, Metrics and Traces in a single pane
____

Without knowing prior to that, this was exactly what I need, so well done marketing:

. Seems to be {8}[FOSS]
. Single solution to address the {25}[three pillars]
. Nice and complete package

That sounds rather too good, but time to put on my wizard hat and to check the brief.
Before messing with {5}[Docker], I checked the documentation and discovered an architecture
overview and this looks like they hold their part of the bargain:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 20
skinparam ranksep 20

rectangle "App (1)" as app #crimson
rectangle "Otel Collector (2)" as otel #green

card "SigNoz" as signoz {
    rectangle "SigNoz Otel Collector (3)" as sigotel
    database "Clickhouse (4)" as clickhouse
    rectangle "Query Service (5)" as query
    rectangle "Alert Manager (6)" as alert
    rectangle "Frontend (7)" as fe
}

app -[hidden]d-> otel

app -r-> sigotel
otel -l-> sigotel

sigotel --> clickhouse
clickhouse --> query
query <--> alert
query <-> fe
{% endplantuml %}
++++
<1> Apps can directly send data to SigNoz
<2> {17}[Otel collectors] can transmit data as well
<3> Internally another custom collector provides the endpoints to receive data
<4> Though I haven't heared of {3}[Clickhouse] either before, but columnar storage sounds about right
<5> Some abstraction to query the actual data
<6> {1}[Alert Manager] keeps tracks and handles all the various alerts - glad they haven't reinvented the wheel
<7> And the shiny bit we've spoken of before

== Collecting data

Once Signoz is running, which basically boils down to calling {4}[docker-compose], a nice starter
question is how to deliver your actual data to it.

{16}[OpenTelemetry] is the defacto standard for that and offers many ways to gather, collect and
transmit data via highly configurable {19}[pipelines].
The only noteworthy thing here to pay attention to the size of the generated logs - which may cause
some headaches as it did for me during my vacation.

While playing with SigNoz I discovered it doesn't connect each of its containers separately to an
{15}[OpenTelemetry Collector].footnote:[otelcol in short], but passes this task entirely to a
container with {14}[logspout].

After a quick glance at the {10}[Github] page marketing did its thing again:

[quote,https://github.com/gliderlabs/logspout]
____
Logspout is a log router for Docker containers that runs inside Docker. It attaches to all
containers on a host, then routes their logs wherever you want. It also has an extensible module
system.
____

Alright, this still sounds like a splendid idea and is exactly we do in the example.
In fact, there isn't much we have to configure at all:

. {5}[Docker] needs a minimal config to get us started:
+
[source,yaml]
----
  logspout:
    container_name: todo-logspout
    image: "docker.io/gliderlabs/logspout:latest"
    pull_policy: if_not_present
    volumes: # <1>
      - /etc/hostname:/etc/host_hostname:ro
      - /var/run/docker.sock:/var/run/docker.sock
    command: syslog+tcp://otelcol:2255 # <2>
    depends_on:
      - otelcol
    restart: on-failure
----
<1> Logspout needs access to the Docker socket and hostmapping for convenience
<2> This configures a connection to a receiver of our otelcol instance and comes up next

. And we have to define a receiver in otelcol:
+
[source,yaml]
----
receivers:
  tcplog/docker:
    listen_address: "0.0.0.0:2255"
    operators: # <1>
      - type: regex_parser # <2>
        regex: '^<([0-9]+)>[0-9]+ (?P<timestamp>[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}(\.[0-9]+)?([zZ]|([\+-])([01]\d|2[0-3]):?([0-5]\d)?)?) (?P<container_id>\S+) (?P<container_name>\S+) [0-9]+ - -( (?P<body>.*))?'
        timestamp:
          parse_from: attributes.timestamp
          layout: '%Y-%m-%dT%H:%M:%S.%LZ'
      - type: move # <3>
        from: attributes["body"]
        to: body
      - type: remove
        field: attributes.timestamp
      - type: filter # <4>
        id: logs_filter
        expr: 'attributes.container_name matches "^todo-(postgres|otelcol|logspout)"'
      - type: json_parser
        parse_form: body
----
<1> Operators allow to parse, modify and filter entries
<2> This is the default format of the messages logspout forwards to otelcol
<3> We basically move our content to the actual body of the entry
<4> There might be lots of different containers running, so we limit the entries based on container names

== Pillars in practice

There is plenty of explanation and definition out there, way better than I can ever provide,
but just to recall the three back to our memory:

[cols="1,5"]
|===
h| Logging
| Historical records of system events and errors

h| Tracing
| Visualization of requests flowing through (distributed) systems

h| Metrics
| Numerical data like e.g. performance, response time, memory consumption
|===

=== Logging

The first pillar is probably the easiest and there is also lots of help and reasoning out there,
{{ site.url }}{% post_url 2022-02-18-logging-vs-tracing %}#logging[including this blog].

So best we can do is throw in {27}[zerolog], add some handling in a {9}[Gin-gonic middleware] and
move on:

[source,go]
----
logEvent.Str("client_id", param.ClientIP). // <1>
    Str("correlation_id", correlationId). // <2>
    Str("method", param.Method).
    Int("status_code", param.StatusCode).
    Int("body_size", param.BodySize).
    Str("path", param.Path).
    Str("latency", param.Latency.String()).
    Msg(param.ErrorMessage)
----
<1> The essential mapping magic happens here
<2> A {{ site.url }}{% post_url 2022-02-18-logging-vs-tracing %}#correlation-between-messages[correlation id]
can help to aggregate log messages of the same origin

SigNoz offers lots of different options to search data and if you have any experience with
{12}[Kibana] and the likes you will probably feel right away at home:

image::logs.png[]

There is also no reason to shy away if you require some kind of aggregation and diagrams with
fancy bars:

image::logs-aggregate.png[]

=== Tracing

The second pillar is a slightly different beast and requires special code to enhance and propagate
a trace - this is generally called
{{ site.url }}{% post_url 2022-02-18-logging-vs-tracing %}#tracing[instrumentation].

OpenTelemetry provides the required toolkit to start a tracer and also add {24}[spans]:

[source,go]
----
func (resource *TodoResource) createTodo(context *gin.Context) {
    tracer := otel.GetTracerProvider().Tracer("todo-resource") // <1>
    ctx, span := tracer.Start(context.Request.Context(), "create-todo",
        trace.WithSpanKind(trace.SpanKindServer))
    defer span.End()

    var todo domain.Todo

    if nil == context.Bind(&todo) {
        var err error

        // Fetch id
        todo.UUID, err = resource.idService.GetId(ctx)

        if nil != err {
            context.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})

            span.SetStatus(http.StatusBadRequest, "UUID failed") // <2>
            span.RecordError(err) // <3>

            return
        }

        // Create todo
        if err = resource.todoService.CreateTodo(ctx, &todo); nil != err {
            context.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})

            return
        }
    } else {
        context.JSON(http.StatusBadRequest, "Invalid request payload")

        return
    }

    span.SetStatus(http.StatusCreated, "Todo created")
    span.SetAttributes(attribute.Int("id", todo.ID), attribute.String("uuid", todo.UUID)) // <4>

    context.JSON(http.StatusCreated, todo)
}
----
<1> This creates a {26}[tracer] based on the current context
<2> {24}[Spans] as working unit of a trace can include a status
<3> Error messages can also be thrown in
<4> And they can also include different types of general {23}[span attributes]

The above code calls the `id-service` and demonstrates how traces can be continued and passed
between service boundaries:

[source,go]
----
func (service *IdService) GetId(ctx context.Context) (string, error) {
    tracer := otel.GetTracerProvider().Tracer("todo-service")
    _, span := tracer.Start(ctx, "get-id")
    defer span.End()

    response, err := otelhttp.Get(ctx, fmt.Sprintf("http://%s/id",
        utils.GetEnvOrDefault("APP_ID_HOST_PORT", "localhost:8081"))) // <1>

    if err != nil {
        return "", err
    }

    jsonBytes, _ := io.ReadAll(response.Body)

    var reply IdServiceReply

    err = json.Unmarshal(jsonBytes, &reply)

    if err != nil {
        return "", err
    }

    return reply.UUID, nil
}
----
<1> The {18}[otelhttp] package makes it really easy to propagate traces

When everything is set up correctly propagated traces look like this:

image::traces.png[]

=== Metrics

The last pillar is one of the most interesting and probably the most troublesome, since there
is no easy recipe what could and what should be done.

Metrics can generally be of following {28}[types]:

[cols="1,5"]
|===
h| Counter
| A simple monotonically increasing counter which can be reset

h| Gauge
| A single value that can go arbitrarily up and down

h| Histogram
| A time series of counter values and a sum

h| Summary
| A histogram with a sum and quantile over a sliding window
|===

This allows a broad range of measurements like the count of requests or the avg latency between
each of them and has to be figured out for each service or rather service landscape individually.

Still, when there are metrics they can be displayed on dashboards like this:

image::metrics.png[]

=== Alerts

Although not directly related to the three pillars, {2}[alerts] are a nice mechanic to define
thresholds and intervals to receive notification over various kind of channels.

The {6}[documentation] is as usual quite nice and there isn't much to add here, besides the fact
a paid subscription is required to connect SigNoz to teams.
There is also a way to fallback to {20}[Power Automate], unfortunately this requires another
subscription.

A little hack is to use connectors for {21}[Prometheus], but please consider supporting the good work of the
folks of SigNoz:

<https://github.com/prometheus-msteams/prometheus-msteams>

image::alerts.png[]

== Conclusion

SigNoz is a great alternative to the established different solutions like EFK or {29}[Grafana] in a
well-rounded package.
It is easy to install and so far as I can say easy to maintain and definitely worth a try.

All examples can be found here:

<https://github.com/unexist/showcase-signoz-golang>
