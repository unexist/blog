---
layout: post
title: Spark vs MapReduce
description: This third blog post of the Big Data series compares the computing model of MapReduce with the features of options of Apache Spark.
#date: %%%DATE%%%
#last_updated: %%%DATE%%%
author: Christoph Kappel
tags: hadoop mapreduce spark versus showcase
categories: tech
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/big_data_and_spark
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/big_data_and_spark
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

////
https://www.goodreads.com/book/show/38467996-spark
https://sparkbyexamples.com/
https://www.python.org/
https://www.r-project.org/
https://mesos.apache.org/
https://kubernetes.io/
https://spark.apache.org/
////

Welcome to round three of our journey through the mysteries world of [Big Data][].

After our first steps with the computing model of [MapReduce][], we are going to focus
on a string contender of it, learn about its strengths and have a look at a few more examples - so
let us say hello to [Apache Spark][].

TIP: If you missed the previous posts just have a look here
     <https://unexist.blog/big-data/2023/10/27/big-data.html> and here
     <https://unexist.blog/big-data/2024/01/05/mapreduce.html>

== What is Spark?

Like MapReduce, Spark is an open-source framework geared towards processing of large quantities of
data for e.g. analytics.
It has different modes of operation and can run as standalone application, use [Mesos][] and [Kubernetes][]
or hop on Hadoop's [YARN][] and directly access data from HDFS.

So far nothing really new, but there is one major difference which make the amazing
processing speed possible:
Spark isn't tied to the two-stage paradigm of MapReduce and can cache data in-memory
between calls directly on the cluster to speed up repeated access to the same data.


On a high-level, Spark consists of following main components:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

top to bottom direction
skinparam linetype ortho
skinparam nodesep 20
skinparam ranksep 20

card "Driver program (1)" as driver {
  rectangle "Spark context" as context
}

card "Worker node (2)" as worker {
    rectangle "RDDS (3) and DAG (4) Scheduler" as sched

    rectangle "Cluster manager (5)" as manager

    card "Executor (6)" as exec {
        rectangle "Cache" as cache
        file "Task" as task1
        file "Task" as task2
    }
}

rectangle "Hadoop HDFS (7)" as hdfs

driver --> worker
sched --> manager
sched -[hidden]d-> exec

manager -d-> exec
worker -> hdfs

context -[hidden]d-> sched
cache -[hidden]> task1
task1 -[hidden]> task2
task2 -[hidden]-> hdfs
{% endplantuml %}
++++
<1> The **driver** is the master process that coordinates the workers and overseas the tasks
<2> A **cluster manager** manages the communication with the worker nodes
<3> **Worker nodes** handle Spark tasks and assigns partions (units of work) to executors
<4> **Resilient Distributed Datasets (RDD)** enable re-checks of data in an event of failure and
act as immutable data store
<5> A **Directed Acyclic Graph (DAG)** is used as the internal presentation of the steps of each job
<6> **Executors** are the working horse of Spark, directly execute jobs and cache the data on the outset
<7> And HDFS is used as underlying storage

=== Modes of execution

- Cluster mode
- Client mode
- Local mode

=== Examples

==== Spark shell

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 30
skinparam ranksep 30

{% raw %}
file in1 [
{{json
  {
    "title":"string","description":"string","done":false,"dueDate":{"start":"2021-05-07","due":"2021-05-07"},"id":0
  }
}}
]
{% endraw %}
{% endplantuml %}
++++

[source,shell]
----
scala> import spark.implicits._
import spark.implicits._

scala> val todoDF = spark.read.json("/home/unexist/todo.json")
todoDF: org.apache.spark.sql.DataFrame = [description: string, done: boolean ... 3 more fields]

scala> todoDF.printSchema()
root
 |-- description: string (nullable = true)
 |-- done: boolean (nullable = true)
 |-- dueDate: struct (nullable = true)
 |    |-- due: string (nullable = true)
 |    |-- start: string (nullable = true)
 |-- id: long (nullable = true)
 |-- title: string (nullable = true)

scala> todoDF.createOrReplaceTempView("todo")

scala> val idDF = spark.sql("SELECT id, title, done FROM todo WHERE id = 0")
idDF: org.apache.spark.sql.DataFrame = [description: string, done: boolean ... 3 more fields]

scala> idDF.show()
+---+------+-----+
| id| title| done|
+---+------+-----+
|  0|string|false|
+---+------+-----+
----

==== Kafka streaming

[source,scala]
----
object TodoSparkSink {
    def main(args: Array[String]): Unit = {
        val sparkConf = new SparkConf() // <1>

        sparkConf.set("packages", "org.apache.iceberg:iceberg-spark-runtime-3.3_2.13:1.1.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1")
        sparkConf.set("spark.sql.catalog.todo_catalog", "org.apache.iceberg.spark.SparkCatalog")
        sparkConf.set("spark.sql.catalog.todo_catalog.type", "hadoop")
        sparkConf.set("spark.sql.catalog.todo_catalog.warehouse", "hdfs://localhost:9000/warehouse")

        val spark = SparkSession // <2>
            .builder()
            .config(sparkConf)
            .appName("TodoSparkSink")
            .getOrCreate()

        import spark.implicits._

        val df = spark.readStream // <3>
            .format("kafka")
            .option("kafka.bootstrap.servers", "localhost:9092")
            .option("subscribe", "todo_created")
            .option("checkpointLocation", "/tmp/checkpoint")
            .load()

    val dataFrame = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
    val resDF = dataFrame.as[(String, String)].toDF("key", "value")

    resDF.writeStream // <4>
      .format("console")
      .outputMode("complete")
      .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))
      .option("path", "todo_catalog.spark.messages")
      .start()

    spark.streams.awaitAnyTermination() // <5>
    spark.streams.resetTerminated()
    }
}
----
<1> Pass the necessary configuration
<2> Create the Spark session
<3> Read the Kafka stream from given server and topic
<4> Write the stream back to a file of the catalog
<5> Wait until everything is done and exit

[source,shell]
----
$ @spark-submit --master spark://localhost:7077 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2 \
    --conf spark.executorEnv.JAVA_HOME=/opt/java/openjdk \
    -- conf spark.yarn.appMasterEnv.JAVA_HOME=/opt/java/openjdk \
    --conf spark.sql.streaming.checkpointLocation=/tmp/checkpoint \
    --name todosink \
    --deploy-mode cluster
    --num-executors 1 \
    --class dev.unexist.showcase.todo.TodoSparkSink \
    hdfs://localhost:9000/jars/todo-spark-sink-0.1.jar
----

[source,shell]
----
[INFO] --- jar:3.3.0:jar (default-jar) @ todo-mapreduce ---
[INFO] Building jar: /Users/unexist/projects/showcase-hadoop-cdc-quarkus/todo-spark-sink/target/todo-spark-sink-0.1.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  1.576 s
[INFO] Finished at: 2024-03-10T17:13:22+01:00
----

== Conclusion

Both frameworks are [FOSS][] and free to use, but there are some key
differences:

|===
| Difference | MapReduce | Spark
| Processing speed
| Depends on the implementations; can be slow
| Spark utilizes memory caching and is much faster

| Processing paradigm
| Designed for batch processing
| Spark supports processing of real-time data with [Spark Streaming][]

| Ease of use
| Strong programming experience in [Java][] is required
| Spark supports multiple programming languages like [R][], [Python][] or [Spark SQL][]

| Integration
| Primarily designed to work with [HDFS]]
| Spark has an extensive ecosystem and integrates well with other Big Data tools
|===

All examples can be found here:

<https://github.com/unexist/showcase-hadoop-cdc-quarkus/tree/master/todo-mapreduce>

[bibliography]
== Bibliography

* [[[sparkdef]]] Bill Chambers, Matei Zaharia, Spark: The Definitive Guide: Big Data Processing Made Easy, O'Reilly 2018