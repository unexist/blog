---
layout: post
title: Spark vs MapReduce
description: This third blog post of the Big Data series introduces Apache Spark and compares it with the features and the computing model of MapReduce.
#date: %%%DATE%%%
#last_updated: %%%DATE%%%
author: Christoph Kappel
tags: hadoop mapreduce spark versus showcase
categories: tech
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/big_data_and_spark
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/big_data_and_spark
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

////
https://www.goodreads.com/book/show/38467996-spark
https://sparkbyexamples.com/
https://www.scala-lang.org/
https://www.python.org/
https://www.r-project.org/
https://mesos.apache.org/
https://kubernetes.io/
https://spark.apache.org/
https://spark.apache.org/docs/latest/cluster-overview.html
////

I've come a bit late to [Big Data][] game and there is no surprise that even things like
[MapReduce][] have some strong contenders even on its own platform.

During the course of this third post we are going to have a look at probably the strongest
contender [Apache Spark][] and its general architecture.
Afterward we tinker with a few examples and conclude with a comparison to the original
computing model of [MapReduce][].

Ready? Onwards!

TIP: If you've missed the previous posts just have a look over here
     <https://unexist.blog/big-data/2023/10/27/big-data.html> and here
     <https://unexist.blog/big-data/2024/01/05/mapreduce.html>.

== What is Spark?

Like MapReduce, Spark is an open-source distributed framework geared towards processing of
large quantities of data for e.g. analytics in a rather unified approach.
It provides a whole platform for writing Big Data applications under a single umbrella and offers
a wide range of support for existing technologies.

It can make use of [cluster managers][] like [Hadoop YARN][], [Apache Mesos][] or [Kubernetes][],
but also supports running in standalone mode even on a single machine.

And this unification idea doesn't stop at the persistence level:
There is support for traditional storage systems including but not limited to [AWS S3][] and
obviously [HDFS][], but it also covers messaging systems like [Kafka[][].

All of these components integrate seamless into the same application and can be assembled from
different API in its primary language [Scala][], but also in the supported programming languages
[Java[], [Python][], [SQL][] and [R][].

This powerful combination allows a great deal of different data tasks ranging from simple data
loading, SQL queries and up to streaming computation.

=== Architecture overview

On a high-level, Spark consist of following main components:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 5
skinparam ranksep 5

together {
    card "Driver program (1)" as driver {
      rectangle "Spark context" as context
    }

    card "Execution engine (2)" as engine {
        rectangle "RDD" as rdd
        rectangle "DAG" as dag
    }
}

card "Cluster manager (3)" as cluster {
    rectangle "Standalone"
    rectangle "Apache Mesos"
    rectangle "Kubernetes"
    rectangle "Hadoop YARN"
}

together {
    card "Worker node (4)" as worker {
          card "Executor (5)" as exec {
              rectangle "Cache (6)" as cache
              file "Task" as task1
              file "Task" as task2
          }
    }

    card "Storage (7)" as storage {
        rectangle "Hadoop HDFS" as hdfs
        rectangle "AWS S3" as s3
    }
}

driver -r--> engine
engine -d--> cluster
cluster -r--> worker
worker -d--> storage

driver -[hidden]d-> worker
cluster -[hidden]r-> worker
worker -[hidden]d-> storage

rdd -[hidden]r-> dag

cache -[hidden]r-> task1
task1 -[hidden]r-> task2

{% endplantuml %}
++++
<1> The **driver** runs and coordinates the main program, maintains state, handles input and
analyzes, distributes and schedules work.
<2> Inside of the **execution engine** changes on immutable
**Resilient Distributed Datasets (RDD)** are translated into a **Directed Acyclic Graph (DAG)**,
split into stages and ultimately into tasks
<3> A **cluster manager** manages and schedules tasks and keeps track of the resources
<4> **Worker nodes** execute the tasks and assign partions (units of work) to executors
<5> **Executors** are the working horse of Spark, directly execute jobs and cache the data on the outset
<6> This is actually the secret ingredient - the **cache** heavily speeds up processing
<7> And at the bottom is a storage layer for the results

== Spark applications

Spark applications can be run in local or **client mode** in a single [JVM][] instance,
which is perfectly suited for first steps with tools like [pyspark][].
In this mode the driver stays where **spark-submit** actually runs:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 20
skinparam ranksep 20

card "Development environment" as dev {
    rectangle "Driver" as driver
    rectangle "Spark context" as context
}

card "Cluster" as cluster {
    rectangle "Master" as master
    rectangle "Worker" as worker
}

dev -> cluster

dev -[hidden]> driver
master -[hidden]> worker
{% endplantuml %}
++++

This is in contrast to the **cluster mode**, which utilizes cluster technology and moves the
driver inside the cluster:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}
skinparam linetype ortho
skinparam nodesep 20
skinparam ranksep 20
card "Development environment" as dev {
    rectangle "Spark context" as context
}

card "Cluster" as cluster {
    rectangle "Driver" as driver
    rectangle "Master" as master
    rectangle "Worker" as worker
}

dev -> cluster

dev -[hidden]> driver
master -[hidden]> worker
{% endplantuml %}
++++

Setting up a cluster takes some time and pain, so for the remainder of this blog
we just stay in **standalone** mode, which provides more than enough prowess for us.

TIP: If you want to run the examples locally here is [Dockerfile][] for a kickstart:
<https://github.com/unexist/showcase-hadoop-cdc-quarkus/blob/master/podman/spark/Dockerfile>

=== Examples

Typically examples inside of this blog revolve around a simple [todo application][], but for all
practical means the data model of todo entries is more than enough.
So for the following examples we just re-use the data structured which can be seen here flattend
out:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 30
skinparam ranksep 30

{% raw %}
file in0 [
{{json
  {
    "title":"string","description":"string","done":false,"dueDate":{"start":"2020-05-07","due":"2021-05-07"},"id":0
  }
}}
]
{% endraw %}
{% endplantuml %}
++++

TIP: Data like this can also be easily written to an underlying HDFS store:
<https://github.com/unexist/showcase-hadoop-cdc-quarkus/blob/master/todo-service-hadoop>

==== Spark shell

The Spark shell acts as a kind of Scala [REPL][] and is an ideal start for experimentation with the API and the
building blocks.

[source,shell]
----
scala> import spark.implicits._
import spark.implicits._

scala> val todoDF = spark.read.json("/home/unexist/todo.json") // <1>
todoDF: org.apache.spark.sql.DataFrame = [description: string, done: boolean ... 3 more fields]

scala> todoDF.printSchema() // <2>
root
 |-- description: string (nullable = true)
 |-- done: boolean (nullable = true)
 |-- dueDate: struct (nullable = true)
 |    |-- due: string (nullable = true)
 |    |-- start: string (nullable = true)
 |-- id: long (nullable = true)
 |-- title: string (nullable = true)

scala> todoDF.createOrReplaceTempView("todo") // <3>

scala> val idDF = spark.sql("SELECT id, title, done FROM todo WHERE id = 0") // <4>
idDF: org.apache.spark.sql.DataFrame = [description: string, done: boolean ... 3 more fields]

scala> idDF.show() // <5>
+---+------+-----+
| id| title| done|
+---+------+-----+
|  0|string|false|
+---+------+-----+
----
<1> The REPL creates a [Spark session][] automatically and we can directly start ingesting JSON data
<2> Spark knows how to handle JSON and provides us a matching [DataFrame][]
<3> Dataframes are mainly simple data structures and can be easily used to create the [SQL view][] **todo**
<4> Once created the view can be accessed like any normal view with SQL
<5> Evaluations of dataframes are lazy and evaluated only when required like to generate output

==== Kafka streaming

[source,scala]
----
object TodoSparkSinkSimple {
    def main(args: Array[String]): Unit = {
        val sparkConf = new SparkConf() // <1>

        sparkConf.set("packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1")

        val spark = SparkSession // <2>
            .builder()
            .config(sparkConf)
            .appName("TodoSparkSinkSimple")
            .getOrCreate()

        import spark.implicits._

        val df = spark.readStream // <3>
            .format("kafka")
            .option("kafka.bootstrap.servers", "localhost:9092")
            .option("subscribe", "todo_created")
            .option("checkpointLocation", "/tmp/checkpoint")
            .load()

    val dataFrame = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
    val resDF = dataFrame.as[(String, String)].toDF("key", "value")

    resDF.writeStream // <4>
      .format("console")
      .outputMode("complete")
      .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))
      .option("path", "todo_catalog.spark.messages")
      .start()

    spark.streams.awaitAnyTermination() // <5>
    spark.streams.resetTerminated()
    }
}
----
<1> Pass the necessary configuration
<2> Create the Spark session
<3> Read the Kafka stream from given server and topic
<4> Write the stream back to a file of the catalog
<5> Wait until everything is done and exit

The compilation of the jar files and rolling the package is a breeze:

[source,shell]
----
$ mvn clean package
...
[INFO] --- jar:3.3.0:jar (default-jar) @ todo-spark-sink ---
[INFO] Building jar: /home/unexist/projects/showcase-hadoop-cdc-quarkus/todo-spark-sink/target/todo-spark-sink-0.1.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  20.348 s
[INFO] Finished at: 2024-03-19T16:07:05+01:00
[INFO] ------------------------------------------------------------------------
----

[source,shell]
----
$ spark-submit --master spark://localhost:7077 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.executorEnv.JAVA_HOME=/opt/java/openjdk \
    --conf spark.yarn.appMasterEnv.JAVA_HOME=/opt/java/openjdk \
    --conf spark.sql.streaming.checkpointLocation=/tmp/checkpoint \
    --name todosink \
    --deploy-mode client \
    --num-executors 1 \
    --class dev.unexist.showcase.todo.TodoSparkSinkSimple \
	./todo-spark-sink/target/todo-spark-sink-0.1.jar
----



== Conclusion

Both frameworks are [FOSS][] and free to use, but there are some key
differences:

|===
| Difference | MapReduce | Spark
| Processing speed
| Depends on the implementations; can be slow
| Spark utilizes memory caching and is much faster

| Processing paradigm
| Designed for batch processing
| Spark supports processing of real-time data with [Spark Streaming][]

| Ease of use
| Strong programming experience in [Java][] is required
| Spark supports multiple programming languages like Python, Java, Scala and R

| Integration
| Primarily designed to work with [HDFS]]
| Spark has an extensive ecosystem and integrates well with other Big Data tools
|===

All examples can be found here:

<https://github.com/unexist/showcase-hadoop-cdc-quarkus/tree/master/todo-mapreduce>

[bibliography]
== Bibliography

* [[[sparkdef]]] Bill Chambers, Matei Zaharia, Spark: The Definitive Guide: Big Data Processing Made Easy, O'Reilly 2018
