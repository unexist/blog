---
layout: post
title: Spark vs MapReduce
description: This third blog post of the Big Data series introduces Apache Spark and compares it with the features and the computing model of MapReduce.
#date: %%%DATE%%%
#last_updated: %%%DATE%%%
author: Christoph Kappel
tags: hadoop mapreduce spark versus showcase
categories: tech
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/big_data_and_spark
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/big_data_and_spark
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

////
https://www.goodreads.com/book/show/38467996-spark
https://sparkbyexamples.com/
https://www.scala-lang.org/
https://www.python.org/
https://www.r-project.org/
https://mesos.apache.org/
https://kubernetes.io/
https://spark.apache.org/
////

I've come a bit late to [Big Data][] game and there is no surprise that even things like
[MapReduce][] have some strong contenders even on its own platform.
[Apache Spark][] is probably one of the strongest contender and provides a unified approach
to the whole idea.

During the course of this third post we are going to have a look at the general architecture of
Spark, talk about differences to the original computing model of [MapReduce][] and tinker with a
few examples.

Ready? Onwards!

TIP: If you missed the previous posts just have a look here
     <https://unexist.blog/big-data/2023/10/27/big-data.html> and here
     <https://unexist.blog/big-data/2024/01/05/mapreduce.html>.

== What is Spark?

Like MapReduce, Spark is an open-source framework geared towards processing of large quantities of
data for e.g. analytics and as initially mentioned follows a unified approach.
It provides a whole platform for writing Big Data applications and offers a wide range of
different data tasks starting from simple data loading, SQL queries and up to streaming computation.

All of these components integrate seemingly into the same application and can be assembled from
different API in its primary language [Scala][], but also in the supported programming languages
[Java[], [Python][], [SQL][] and [R][].

This unification idea doesn't stop at the persistence level:
There is a wide support of different systems including but not limited to [AWS S3][],
[Apache Cassandra][], and obviously HDFS, but also includes messaging systems like [Kafka[][].

=== Spark architecture

On a high-level, Spark application consist of following main components:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 20
skinparam ranksep 20

together {
    card "Driver program (1)" as driver {
      rectangle "Spark context" as context
    }

    rectangle "RDDS (2) and DAG (3) Scheduler" as sched
    rectangle "Cluster manager (4)" as manager

    card "Worker node (5)" as worker {
          card "Executor (6)" as exec {
              rectangle "Cache" as cache
              file "Task" as task1
              file "Task" as task2
          }
    }

    rectangle "Hadoop HDFS (7)" as hdfs
}

driver -d-> sched
sched -d-> manager
manager -d-> worker
worker -d-> hdfs

cache -[hidden]r-> task1
task1 -[hidden]r-> task2
{% endplantuml %}
++++
<1> The **driver** is the master process that coordinates the workers and overseas the tasks
<2> **Worker nodes** handle Spark tasks and assigns partions (units of work) to executors
<3> **Resilient Distributed Datasets (RDD)** enable re-checks of data in an event of failure and
act as immutable data store
<4> A **cluster manager** manages the communication with the worker nodes and keeps track of
the resources
<5> A **Directed Acyclic Graph (DAG)** is used as the internal presentation of the steps of each job
<6> **Executors** are the working horse of Spark, directly execute jobs and cache the data on the outset
<7> And here, HDFS is used as underlying storage

== Spark applications

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}
skinparam linetype ortho
skinparam nodesep 20
skinparam ranksep 20

card "Development environment" as dev {
    rectangle "Driver" as driver
    rectangle "Spark context" as context
}

card "Cluster" as cluster {
    rectangle "Master" as master
    rectangle "Worker" as worker
}

dev -> cluster

dev -[hidden]> driver
master -[hidden]> worker
{% endplantuml %}
++++

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}
skinparam linetype ortho
skinparam nodesep 20
skinparam ranksep 20
card "Development environment" as dev {
    rectangle "Spark context" as context
}

card "Cluster" as cluster {
    rectangle "Driver" as driver
    rectangle "Master" as master
    rectangle "Worker" as worker
}

dev -> cluster

dev -[hidden]> driver
master -[hidden]> worker
{% endplantuml %}
++++

=== Examples

Spark applications can be run either in a **cluster mode** and make use of many worker nodes and
executors, but also feel at home in a **local mode**, with all processes running on the same
machine and provides us with enough prowess for the next two examples.

Typically examples inside of this blog revolve around a simple [todo application][], but for all
practical means the data model of todo entries is more than enough.

Here is the flattened out [JSON][] structure to make it easier to understand what happens in the
next chapters:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 30
skinparam ranksep 30

{% raw %}
file in0 [
{{json
  {
    "title":"string","description":"string","done":false,"dueDate":{"start":"2020-05-07","due":"2021-05-07"},"id":0
  }
}}
]
{% endraw %}
{% endplantuml %}
++++

TIP: If you still need an example service go over here and have a look at how the data can be written to the
HDFS store:
<https://github.com/unexist/showcase-hadoop-cdc-quarkus/blob/master/todo-service-hadoop>

==== Spark shell

The Spark shell acts as a kind of Scala [REPL][] and is an ideal start for experimentation with the building
blocks and API.

[source,shell]
----
scala> import spark.implicits._
import spark.implicits._

scala> val todoDF = spark.read.json("/home/unexist/todo.json") // <1>
todoDF: org.apache.spark.sql.DataFrame = [description: string, done: boolean ... 3 more fields]

scala> todoDF.printSchema() // <2>
root
 |-- description: string (nullable = true)
 |-- done: boolean (nullable = true)
 |-- dueDate: struct (nullable = true)
 |    |-- due: string (nullable = true)
 |    |-- start: string (nullable = true)
 |-- id: long (nullable = true)
 |-- title: string (nullable = true)

scala> todoDF.createOrReplaceTempView("todo") // <3>

scala> val idDF = spark.sql("SELECT id, title, done FROM todo WHERE id = 0") // <4>
idDF: org.apache.spark.sql.DataFrame = [description: string, done: boolean ... 3 more fields]

scala> idDF.show() // <5>
+---+------+-----+
| id| title| done|
+---+------+-----+
|  0|string|false|
+---+------+-----+
----
<1> The REPL creates a [Spark session][] automatically and we can directly start ingesting JSON data
<2> Spark knows how to handle JSON and provides us a matching [DataFrame][]
<3> Dataframes are mainly simple data structures and can be easily used to create the [SQL view][] **todo**
<4> Once created the view can be accessed like any normal view with SQL
<5> Evaluations of dataframes are lazy and evaluated only when required like to generate output

==== Kafka streaming

[source,scala]
----
object TodoSparkSinkSimple {
    def main(args: Array[String]): Unit = {
        val sparkConf = new SparkConf() // <1>

        sparkConf.set("packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1")

        val spark = SparkSession // <2>
            .builder()
            .config(sparkConf)
            .appName("TodoSparkSinkSimple")
            .getOrCreate()

        import spark.implicits._

        val df = spark.readStream // <3>
            .format("kafka")
            .option("kafka.bootstrap.servers", "localhost:9092")
            .option("subscribe", "todo_created")
            .option("checkpointLocation", "/tmp/checkpoint")
            .load()

    val dataFrame = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
    val resDF = dataFrame.as[(String, String)].toDF("key", "value")

    resDF.writeStream // <4>
      .format("console")
      .outputMode("complete")
      .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))
      .option("path", "todo_catalog.spark.messages")
      .start()

    spark.streams.awaitAnyTermination() // <5>
    spark.streams.resetTerminated()
    }
}
----
<1> Pass the necessary configuration
<2> Create the Spark session
<3> Read the Kafka stream from given server and topic
<4> Write the stream back to a file of the catalog
<5> Wait until everything is done and exit

The compilation of the jar files and rolling the package is a breeze:

[source,shell]
----
$ mvn clean package
...
[INFO] --- jar:3.3.0:jar (default-jar) @ todo-spark-sink ---
[INFO] Building jar: /home/unexist/projects/showcase-hadoop-cdc-quarkus/todo-spark-sink/target/todo-spark-sink-0.1.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  20.348 s
[INFO] Finished at: 2024-03-19T16:07:05+01:00
[INFO] ------------------------------------------------------------------------
----

[source,shell]
----
$ spark-submit --master spark://localhost:7077 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.executorEnv.JAVA_HOME=/opt/java/openjdk \
    --conf spark.yarn.appMasterEnv.JAVA_HOME=/opt/java/openjdk \
    --conf spark.sql.streaming.checkpointLocation=/tmp/checkpoint \
    --name todosink \
    --deploy-mode client \
    --num-executors 1 \
    --class dev.unexist.showcase.todo.TodoSparkSinkSimple \
	./todo-spark-sink/target/todo-spark-sink-0.1.jar
----



== Conclusion

Both frameworks are [FOSS][] and free to use, but there are some key
differences:

|===
| Difference | MapReduce | Spark
| Processing speed
| Depends on the implementations; can be slow
| Spark utilizes memory caching and is much faster

| Processing paradigm
| Designed for batch processing
| Spark supports processing of real-time data with [Spark Streaming][]

| Ease of use
| Strong programming experience in [Java][] is required
| Spark supports multiple programming languages like Python, Java, Scala and R

| Integration
| Primarily designed to work with [HDFS]]
| Spark has an extensive ecosystem and integrates well with other Big Data tools
|===

All examples can be found here:

<https://github.com/unexist/showcase-hadoop-cdc-quarkus/tree/master/todo-mapreduce>

[bibliography]
== Bibliography

* [[[sparkdef]]] Bill Chambers, Matei Zaharia, Spark: The Definitive Guide: Big Data Processing Made Easy, O'Reilly 2018
