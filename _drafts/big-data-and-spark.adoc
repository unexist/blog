---
layout: post
title: Spark vs MapReduce
description: This third blog post of the Big Data series introduces Apache Spark and compares it with the features and the computing model of MapReduce.
#date: %%%DATE%%%
#last_updated: %%%DATE%%%
author: Christoph Kappel
tags: hadoop mapreduce spark versus showcase
categories: tech
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/big_data_and_spark
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/big_data_and_spark
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

////
https://www.goodreads.com/book/show/38467996-spark
https://sparkbyexamples.com/
https://www.scala-lang.org/
https://www.python.org/
https://www.r-project.org/
https://mesos.apache.org/
https://kubernetes.io/
https://spark.apache.org/
////

I've started a bit late with the [Big Data][] game and there is no surprise that even things like
[MapReduce][] have some strong contenders even on its own platform.
[Apache Spark][] is probably one of the the strongest contender and provides a unified approach
to the whole idea.

So during the course of this third post we are going to learn about the general architecture of
Spark, talk about differences to the original computing model of [MapReduce][] and have a look
at a few examples.

Ready? Onwards!

TIP: If you missed the previous posts just have a look here
     <https://unexist.blog/big-data/2023/10/27/big-data.html> and here
     <https://unexist.blog/big-data/2024/01/05/mapreduce.html>.

== What is Spark?

Like MapReduce, Spark is an open-source framework geared towards processing of large quantities of
data for e.g. analytics and as initially mentioned follows a unified approach.
It provides a whole platform for writing of Big Data applications and offers a wide range of
different data tasks like simple loading of data to SQL queries to streaming computation.

All of these components integrate seemingly inside the same application and can be assembled from
different API in the supported programming languages [Python[], [Java][], [Scala][] and [R][].

It has different modes of operation and can run as standalone application, use [Mesos][] and [Kubernetes][]
or hop on Hadoop's [YARN][] and directly access data from HDFS.

So far nothing really new, but there is one major difference which make the amazing
processing speed possible:
Spark isn't tied to the two-stage paradigm of MapReduce and can cache data in-memory
between calls directly on the cluster to speed up repeated access to the same data.


On a high-level, Spark consists of following main components:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 20
skinparam ranksep 20

together {
    card "Driver program (1)" as driver {
      rectangle "Spark context" as context
    }

    rectangle "RDDS (2) and DAG (3) Scheduler" as sched
    rectangle "Cluster manager (4)" as manager

    card "Worker node (5)" as worker {
          card "Executor (6)" as exec {
              rectangle "Cache" as cache
              file "Task" as task1
              file "Task" as task2
          }
    }

    rectangle "Hadoop HDFS (7)" as hdfs
}

driver -d-> sched
sched -d-> manager
manager -d-> worker
worker -d-> hdfs

cache -[hidden]r-> task1
task1 -[hidden]r-> task2
{% endplantuml %}
++++
<1> The **driver** is the master process that coordinates the workers and overseas the tasks
<2> **Worker nodes** handle Spark tasks and assigns partions (units of work) to executors
<3> **Resilient Distributed Datasets (RDD)** enable re-checks of data in an event of failure and
act as immutable data store
<4> A **cluster manager** manages the communication with the worker nodes
<5> A **Directed Acyclic Graph (DAG)** is used as the internal presentation of the steps of each job
<6> **Executors** are the working horse of Spark, directly execute jobs and cache the data on the outset
<7> And HDFS is used as underlying storage

=== Examples

==== Spark shell

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 30
skinparam ranksep 30

{% raw %}
file in1 [
{{json
  {
    "title":"string","description":"string","done":false,"dueDate":{"start":"2021-05-07","due":"2021-05-07"},"id":0
  }
}}
]
{% endraw %}
{% endplantuml %}
++++

[source,shell]
----
scala> import spark.implicits._
import spark.implicits._

scala> val todoDF = spark.read.json("/home/unexist/todo.json")
todoDF: org.apache.spark.sql.DataFrame = [description: string, done: boolean ... 3 more fields]

scala> todoDF.printSchema()
root
 |-- description: string (nullable = true)
 |-- done: boolean (nullable = true)
 |-- dueDate: struct (nullable = true)
 |    |-- due: string (nullable = true)
 |    |-- start: string (nullable = true)
 |-- id: long (nullable = true)
 |-- title: string (nullable = true)

scala> todoDF.createOrReplaceTempView("todo")

scala> val idDF = spark.sql("SELECT id, title, done FROM todo WHERE id = 0")
idDF: org.apache.spark.sql.DataFrame = [description: string, done: boolean ... 3 more fields]

scala> idDF.show()
+---+------+-----+
| id| title| done|
+---+------+-----+
|  0|string|false|
+---+------+-----+
----

==== Kafka streaming

[source,scala]
----
object TodoSparkSink {
    def main(args: Array[String]): Unit = {
        val sparkConf = new SparkConf() // <1>

        sparkConf.set("packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1")
        sparkConf.set("spark.sql.catalog.todo_catalog", "org.apache.iceberg.spark.SparkCatalog")
        sparkConf.set("spark.sql.catalog.todo_catalog.type", "hadoop")
        sparkConf.set("spark.sql.catalog.todo_catalog.warehouse", "hdfs://localhost:9000/warehouse")

        val spark = SparkSession // <2>
            .builder()
            .config(sparkConf)
            .appName("TodoSparkSink")
            .getOrCreate()

        import spark.implicits._

        val df = spark.readStream // <3>
            .format("kafka")
            .option("kafka.bootstrap.servers", "localhost:9092")
            .option("subscribe", "todo_created")
            .option("checkpointLocation", "/tmp/checkpoint")
            .load()

    val dataFrame = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
    val resDF = dataFrame.as[(String, String)].toDF("key", "value")

    resDF.writeStream // <4>
      .format("console")
      .outputMode("complete")
      .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))
      .option("path", "todo_catalog.spark.messages")
      .start()

    spark.streams.awaitAnyTermination() // <5>
    spark.streams.resetTerminated()
    }
}
----
<1> Pass the necessary configuration
<2> Create the Spark session
<3> Read the Kafka stream from given server and topic
<4> Write the stream back to a file of the catalog
<5> Wait until everything is done and exit

[source,shell]
----
$ spark-submit --master spark://localhost:7077 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2 \
    --conf spark.executorEnv.JAVA_HOME=/opt/java/openjdk \
    --conf spark.yarn.appMasterEnv.JAVA_HOME=/opt/java/openjdk \
    --conf spark.sql.streaming.checkpointLocation=/tmp/checkpoint \
    --name todosink \
    --deploy-mode cluster \
    --num-executors 1 \
    --class dev.unexist.showcase.todo.TodoSparkSink \
    hdfs://localhost:9000/jars/todo-spark-sink-0.1.jar
----

[source,shell]
----
[INFO] --- jar:3.3.0:jar (default-jar) @ todo-mapreduce ---
[INFO] Building jar: /Users/unexist/projects/showcase-hadoop-cdc-quarkus/todo-spark-sink/target/todo-spark-sink-0.1.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  1.576 s
[INFO] Finished at: 2024-03-10T17:13:22+01:00
----

== Conclusion

Both frameworks are [FOSS][] and free to use, but there are some key
differences:

|===
| Difference | MapReduce | Spark
| Processing speed
| Depends on the implementations; can be slow
| Spark utilizes memory caching and is much faster

| Processing paradigm
| Designed for batch processing
| Spark supports processing of real-time data with [Spark Streaming][]

| Ease of use
| Strong programming experience in [Java][] is required
| Spark supports multiple programming languages like Python, Java, Scala and R

| Integration
| Primarily designed to work with [HDFS]]
| Spark has an extensive ecosystem and integrates well with other Big Data tools
|===

All examples can be found here:

<https://github.com/unexist/showcase-hadoop-cdc-quarkus/tree/master/todo-mapreduce>

[bibliography]
== Bibliography

* [[[sparkdef]]] Bill Chambers, Matei Zaharia, Spark: The Definitive Guide: Big Data Processing Made Easy, O'Reilly 2018
