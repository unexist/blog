---
layout: post
title: Spark vs MapReduce
description: This third blog post of the Big Data series compares the computing model of MapReduce with the features of options of Apache Spark.
#date: %%%DATE%%%
#last_updated: %%%DATE%%%
author: Christoph Kappel
tags: hadoop mapreduce spark versus showcase
categories: tech
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/big_data_and_spark
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/big_data_and_spark
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

////
https://www.goodreads.com/book/show/38467996-spark
https://sparkbyexamples.com/
https://www.python.org/
https://www.r-project.org/
////

Welcome to round three of our journey through the mysteries world of [Big Data][].

After our first steps with the computing model of [MapReduce][], we are going to focus
on a string contender of it, learn about its strengths and have a look at a few examples - so
without further ado let us say hello to [Apache Spark][].

== What is Spark?

Spark is an open-source framework that proce

Here is a simplified view of the architecture:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

left to right direction
skinparam linetype ortho
skinparam nodesep 10
skinparam ranksep 50

card "Driver program" as driver {
  rectangle "Spark context" as context
}

rectangle "Cluster manager" as manager

card "Worker node" as worker {
    card "Executor" as exec {
       file "Task" as task1
       file "Task" as task2
    }
}

context <--> manager
driver <--> exec
manager --> worker

task1 -[hidden]-> task2
{% endplantuml %}
++++

=== Examples

==== Spark shell

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 30
skinparam ranksep 30

{% raw %}
file in1 [
{{json
  {
    "title":"string","description":"string","done":false,"dueDate":{"start":"2021-05-07","due":"2021-05-07"},"id":0
  }
}}
]
{% endraw %}
{% endplantuml %}
++++

[source,shell]
----
scala> import spark.implicits._
import spark.implicits._

scala> val todoDF = spark.read.json("/Users/ces/todo.json")
todoDF: org.apache.spark.sql.DataFrame = [description: string, done: boolean ... 3 more fields]

scala> todoDF.printSchema()
root
 |-- description: string (nullable = true)
 |-- done: boolean (nullable = true)
 |-- dueDate: struct (nullable = true)
 |    |-- due: string (nullable = true)
 |    |-- start: string (nullable = true)
 |-- id: long (nullable = true)
 |-- title: string (nullable = true)

scala> todoDF.createOrReplaceTempView("todo")

scala> val idDF = spark.sql("SELECT id, title, done FROM todo WHERE id = 0")
idDF: org.apache.spark.sql.DataFrame = [description: string, done: boolean ... 3 more fields]

scala> idDF.show()
+---+------+-----+
| id| title| done|
+---+------+-----+
|  0|string|false|
+---+------+-----+
----

==== Kafka streaming

[source,scala]
----
object TodoSparkSink {
    def main(args: Array[String]): Unit = {
        val sparkConf = new SparkConf() // <1>

        sparkConf.set("packages", "org.apache.iceberg:iceberg-spark-runtime-3.3_2.13:1.1.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1")
        sparkConf.set("spark.sql.catalog.todo_catalog", "org.apache.iceberg.spark.SparkCatalog")
        sparkConf.set("spark.sql.catalog.todo_catalog.type", "hadoop")
        sparkConf.set("spark.sql.catalog.todo_catalog.warehouse", "hdfs://localhost:9000/warehouse")

        val spark = SparkSession // <2>
            .builder()
            .config(sparkConf)
            .appName("TodoSparkSink")
            .getOrCreate()

        import spark.implicits._

        val df = spark.readStream // <3>
            .format("kafka")
            .option("kafka.bootstrap.servers", "localhost:9092")
            .option("subscribe", "todo_created")
            .option("checkpointLocation", "/tmp/checkpoint")
            .load()

    val dataFrame = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
    val resDF = dataFrame.as[(String, String)].toDF("key", "value")

    resDF.writeStream // <4>
      .format("console")
      .outputMode("complete")
      .trigger(Trigger.ProcessingTime(1, TimeUnit.MINUTES))
      .option("path", "todo_catalog.spark.messages")
      .start()

    spark.streams.awaitAnyTermination() // <5>
    spark.streams.resetTerminated()
    }
}
----
<1> Pass the necessary configuration
<2> Create the Spark session
<3> Read the Kafka stream from given server and topic
<4> Write the stream back to a file of the catalog
<5> Wait until everything is done and exit

[source,shell]
----
$ @spark-submit --master spark://localhost:7077 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2 \
    --conf spark.executorEnv.JAVA_HOME=/opt/java/openjdk \
    -- conf spark.yarn.appMasterEnv.JAVA_HOME=/opt/java/openjdk \
    --conf spark.sql.streaming.checkpointLocation=/tmp/checkpoint \
    --name todosink \
    --deploy-mode cluster
    --num-executors 1 \
    --class dev.unexist.showcase.todo.TodoSparkSink \
    hdfs://localhost:9000/jars/todo-spark-sink-0.1.jar
----

[source,shell]
----
[INFO] --- jar:3.3.0:jar (default-jar) @ todo-mapreduce ---
[INFO] Building jar: /Users/unexist/projects/showcase-hadoop-cdc-quarkus/todo-spark-sink/target/todo-spark-sink-0.1.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  1.576 s
[INFO] Finished at: 2024-03-10T17:13:22+01:00
----

== Difference between MapReduce and Spark

Both frameworks are [FOSS][] and free to use, but there are some key
differences:

|===
| Difference | MapReduce | Spark
| Processing speed
| Depends on the implementations; can be slow
| Spark utilizes memory caching and is much faster

| Processing paradigm
| Designed for batch processing
| Spark supports processing of real-time data with [Spark Streaming][]

| Ease of use
| Strong programming experience in [Java][] is required
| Spark supports multiple programming languages like [R][], [Python][] or [Spark SQL][]

| Integration
| Primarily designed to work with [HDFS]]
| Spark has an extensive ecosystem and integrates well with other Big Data tools
|===

== Conclusion

All examples can be found here:

<https://github.com/unexist/showcase-hadoop-cdc-quarkus/tree/master/todo-mapreduce>

[bibliography]
== Bibliography

* [[[sparkdef]]] Bill Chambers, Matei Zaharia, Spark: The Definitive Guide: Big Data Processing Made Easy, O'Reilly 2018