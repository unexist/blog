---
layout: post
title: Evolving a format
date: %%%DATE%%%
last_updated: %%%DATE%%%
author: Christoph Kappel
tags: showcase
categories: json jsonschema avro showcase
toc: true
---
////
https://json-schema.org/
https://www.liquid-technologies.com/online-json-to-schema-converter
https://konbert.com/convert/json/to/avro
https://digital-preservation.github.io/csv-schema/
http://thomasburette.com/blog/2014/05/25/so-you-want-to-write-your-own-CSV-code/
https://avro.apache.org/docs/1.11.1/specification/
https://www.json.org/
https://www.goodreads.com/book/show/23463279-designing-data-intensive-applications
////

Modern distributed systems consist of dozens of different services and each of these needs to
communicate with others to fulfill tasks.
Without a well-known and understood format things can become messy, once data needs to go down to
the wire.

The first part of this article compares the textual formats [CSV][] and [JSON][] with the
binary-based [Avro][] and lays out how both can be evolved on a format level.
And the second part explains how the same can be archived with versions on examples based on
[REST][].

NOTE: This articles uses the Todo object usually found in many posts of this this blog - if you
haven't seen it before you can find an [OpenAPI][] found:
<https://blog.unexist.dev/redoc/#tag/Todo>

== Picking a format

Picking a format should be fairly easy:
Anything that can be parsed on the receiving end **is** a proper format, so we just serialize
(or encode) our object values separated by commas ([CSV][]):

[source,csv]
----
First,Bla,true,2022-11-21,0
----

On the implementation side we don't want to needlessly [roll our own CSV code][], so a after quick
check we settle for any of the hopefully mature CSV-parser our programming language provides.
And once wired up and deployed, our solutions works splendidly, until a change is necessary.

So far there weren't any citation of old Greeks in this blog - about time to change that:

[quote,Heraclitus,https://www.reference.com/world-view/said-only-thing-constant-change-d50c0532e714e12b]
Change is the only constant in life.

== Dealing with change

It is probably easy to imagine a scenario that requires a change of our format:

- A new requirement for additional fields
- Some data type needs to be changed
- Removal of data due to regulations
- ..the list is endless!

How do our formats fare with these problems?

=== Good ol' CSV

This is probably a bit unfair, we all know CSV isn't on par with the other formats, but maybe there
is a surprise waiting and we also want stay in line with the format of the post, right?

==== Add or remove fields

Adding or removing fields to and from our original version is really difficult - readers must
be able to match the actual fields and any change (even the order) makes their life miserable.

A straight forward solution here is just to include the names of the fields in a header - this is
pretty common and probably (in)famously known from [Excel][]:

[source,csv]
----
title,description,done,dueDate,id
First,Bla,true,2022-11-21,0
----

NOTE: We ignore the fact, that values itself can also include a comma and assume our lovely
CSV-parser handles theses cases perfectly well.

==== Change of data types

Figuring out the data type of the values is up to the reader, we omit all information about
data types.

This kind of definition can usually be done with a schema, which basically describes the format
including data types and also allows some form of verification of values.

Surprisingly, something like this already exists for CSV, so let me introduce you to
[CSV Schema][].

The schema itself is straight forward and comes with lots of keywords like `positiveInteger` or
`regex` to provide arbitrary regular expressions:

[source,text]
----
version 1.0
@totalColumns 5

title: regex("[-/0-9\w\s,.]+")
description: regex("[-/0-9\w\s,.]+")
done: is("true") or is("false")
dueDate: regex("[0-9]{4}-[0-9]{2}-[0-9]{2}")
id: positiveInteger
----

NOTE: The full specification can be found here: <http://digital-preservation.github.io/csv-schema/csv-schema-1.1.html>

Having a schema that can be used to verify input is nice, but the major advantage is the format is
now specified, documented and can be put under version control.

==== Complex data types

There is no support for complex or nested types at all.

=== Textual with JSON

==== Add or remove fields
==== Change of data types
==== Complex data types

[source,json]
----
{
    "title": "First",
    "description": "Bla",
    "done": true,
    "dueDate": "2022-11-21",
    "id": 0
}
----


[source,json]
----
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
  "properties": {
    "title": {
      "type": "string"
    },
    "description": {
      "type": "string"
    },
    "done": {
      "type": "boolean"
    },
    "dueDate": {
      "type": "string"
    },
    "id": {
      "type": "integer"
    }
  },
  "required": [
    "title",
    "description",
    "done",
    "dueDate",
    "id"
  ]
}
----

=== Avro and the binary

==== Add or remove fields
==== Change of data types
==== Complex data types

[source,avro]
----
{
  "type": "record",
  "name": "Record",
  "fields": [
    {
      "name": "title",
      "type": "string"
    },
    {
      "name": "description",
      "type": "string"
    },
    {
      "name": "done",
      "type": "boolean"
    },
    {
      "name": "dueDate",
      "type": "string"
    },
    {
      "name": "id",
      "type": "long"
    }
  ]
}
----

NOTE: [Martin Kleppman][] compares various binary formats in his seminal book
[Designing Data-Intensive Application][].

== Apply versioning

There are multiple ways to apply versioning here, but let us limit ourselves to the two more common
ones usually found with [REST][].

=== Endpoint versioning

Our first option is to create a new version of our endpoint, by adding the version number to the
endpoint [URI][], which basically allows every kind of tracking and redirection magic:

[source,shell]
----
$ curl -X GET http://blog.unexist.dev/api/1/todos # <1>
----
<1> Set the version via [path parameter][]

|===
| Pro | Con
| Clean separation of the endpoints
| Lots of copy/paste or worse people thinking about [DRY][]

| Usage and therefore deprecation of the endpoint can be tracked e.g. with [PACT][]
|

|
| Further evolution might require a new endpoint
|===

=== Content versioning

And the second option is to serve all versions from a single endpoint by honoring client-provided
preferences here in the form of an [accept header][].
This has the additional benefit of offloading the content negotiation part to the client, so it can
pick the format it understands.

[source,shell]
----
$ curl -X GET -H “Accept: application/vnd.xm.device+json; version=1” http://blog.unexist.dev/api/todos # <1>
----
<1> Set the version via [Accept header][]

|===
| Pro | Con
| Single version of endpoint
| Increases the complexity of the endpoint to include version handling

|
| Difficult to track the actual usage of specific versions without header analysis

| New versions can be easily added and served
|
|===

== Conclusion

Like so often in IT, both options have their merits and depend on what you are really up to.

In big architectures, it can be useful to be able to serve different versions of your messages on
different microservices and keep them really small and simple (see [KISS][]).
PACT can also help here to keep track of the different versions available and also provide insights
to actual use patterns.

From a nitpicking perspective, versioning the actual content is preferable, because you have in
fact just one version of the endpoint - it just serves a different version of your format.
And letting clients pick whatever they support is something that is also deeply ingrained into the
whole REST idea.

So whatever you pick, both options allow the client to select a version, either by route or by
header and the first problem is addressed.

All examples can be found here:

<https://github.com/unexist/showcase-kafka-quarkus>