---
layout: post
title: MapReduce
#date: %%%DATE%%%
#last_updated: %%%DATE%%%
author: Christoph Kappel
tags: hadoop big-data showcase
categories: tech
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/mapreduce
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/mapreduce
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

////
https://mrunit.apache.org/
////

In the previous post of my [Big Data][] series we started with a bit of the technical concepts of
[Hadoop][], briefly covered the computing model and explored how it can be used as a plain file
storage.
This is from a technical point interesting, but storage alone is only half the fun.

So in this follow-up we are going to dive into the world of [MapReduce][], the unit-test frame
[MRUnit][] and a really simple todo example the advantages of the whole idea.

Understanding a bit of the basics is really helpful, so if you haven't read the previous post yet
we will wait - promised.

Ready? So without further ado let us start.

== What is MapReduce?

++++
{% plantuml %}
skinparam linetype ortho
skinparam nodesep 30
skinparam ranksep 30

together {
  file in1 [
{{json
  {
    "title":"string","description":"string","done":false,"dueDate":{"start":"2021-05-07","due":"2021-05-07"},"id":0
  }
}}
  ]

  file in2 [
{{json
  {
    "title":"string","description":"string","done":false,"dueDate":{"start":"2021-05-07","due":"2021-05-07"},"id":1
  }
}}
  ]
}

together {
  rectangle "Map (1)" as map1 #red
  rectangle "Map (1)" as map2 #red
}

together {
  file shuf1 [
2021-05-07 => 0
  ]

  file shuf2 [
2021-05-07 => 1
  ]
}

file out [
2021-06-07 => (0, 1, 2)
]

rectangle "Shuffle (2)" as shuffle #green
rectangle "Reduce (3)" as reduce #blue

in1 -> map1
in2 -> map2

map1 -> shuf1
map2 -> shuf2

shuf1 -> shuffle
shuf2 -> shuffle

shuffle -> reduce

reduce -> out

' Layout

in1 -[hidden]-> in2
map1 -[hidden]-> map2
shuf1 -[hidden]-> shuf2
{% endplantuml %}
++++

=== Mapper

[source,java]
----
public class TodoMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    private final static IntWritable addOne = new IntWritable(1);

    private Text dueDate = new Text();
    private ObjectMapper mapper = new ObjectMapper();

    protected void map(LongWritable key, Text value, Context context)
            throws java.io.IOException, InterruptedException
    {
        try {
            Todo todo = this.mapper.readValue(value.toString(), Todo.class);

            dueDate.set(todo.getDueDate().getDue()
                    .format(DateTimeFormatter.ofPattern(DueDate.DATE_PATTERN)));

            context.write(dueDate, addOne);
        } catch (JsonProcessingException e) {
            /* Do nothing */
        }
    }
}
----

=== Reducer

[source,java]
----
public class TodoReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws java.io.IOException,
            InterruptedException
    {
        int sum = 0;

        for (IntWritable value : values) {
            sum += value.get();
        }

        context.write(key, new IntWritable(sum));
    }
}
----

=== Testing

[source,java]
----
public class TodoMapperReducerTest {
    final static String RECORD =
            "{\"title\":\"string\",\"description\":\"string\",\"done\":false,\"dueDate\":{\"start\":\"2021-05-07\",\"due\":\"2021-05-07\"},\"id\":0}";

    MapDriver<LongWritable, Text, Text, IntWritable> mapDriver;
    ReduceDriver<Text, IntWritable, Text, IntWritable> reduceDriver;
    MapReduceDriver<LongWritable, Text, Text, IntWritable, Text, IntWritable> mapReduceDriver;

    @Before
    public void setUp() {
        TodoMapper mapper = new TodoMapper();
        TodoReducer reducer = new TodoReducer();

        mapDriver = MapDriver.newMapDriver(mapper);
        reduceDriver = ReduceDriver.newReduceDriver(reducer);
        mapReduceDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer);
    }

    @Test
    public void shouldVerifyMapper() throws IOException {
        mapDriver.withInput(new LongWritable(), new Text(RECORD));
        mapDriver.withOutput(new Text("2021-05-07"), new IntWritable(1));
        mapDriver.runTest();
    }

    @Test
    public void shouldVerifyReducer() throws IOException {
        List<IntWritable> values = new ArrayList<IntWritable>();

        values.add(new IntWritable(1));
        values.add(new IntWritable(1));

        reduceDriver.withInput(new Text("2021-05-07"), values);
        reduceDriver.withOutput(new Text("2021-05-07"), new IntWritable(2));
        reduceDriver.runTest();
    }

    @Test
    public void shouldVerfiyMapAndReduce() throws IOException {
        mapReduceDriver.withInput(new LongWritable(), new Text(RECORD));

        List<IntWritable> values = new ArrayList<IntWritable>();

        values.add(new IntWritable(1));
        values.add(new IntWritable(1));

        mapReduceDriver.withOutput(new Text("2021-05-07"), new IntWritable(1));
        mapReduceDriver.runTest();
    }
}
----

== Conclusion

All examples can be found here:

<https://github.com/unexist/showcase-hadoop-cdc-quarkus/>

[bibliography]
== Bibliography

* [[[hadooparch]]] Mark Grover, Ted Malask, Jonathan Seidman, Gwen Shapira, Hadoop Application Architectures, O'Reilly 2015