---
layout: post
title: MapReduce
date: 2023-09-02 16:05:48 +0200
last_updated: 2023-09-02 16:05:48 +0200
author: Christoph Kappel
tags: hadoop big-data mapreduce showcase
categories: tech
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/mapreduce
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/mapreduce
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

In the previous post of my [Big Data][] series we started with a bit of the technical concepts of
[Hadoop][], briefly covered its computing model and explored how it can be used as a plain file
storage.
This is from a technical point interesting, but storage alone is only half the fun.

So in this follow-up we are going to dive into the world of [MapReduce][], the unit-test framework
[MRUnit][] and a really simple [todo][] example to demonstrate the advantages of the whole idea.

While not strictly required to understand why Hadoop is an excellent choice for storage, it really
helps to have a bit of understanding for the course of this article, so if you haven't read the
previous post yet, we will wait - promised:

<https://blog.unexist.dev/big-data/2023/10/27/big-data.html>

Ready? So without further ado let us start.

== What is MapReduce?

One of the major problems with processing large quantities of data is that the processing of it
doesn't necessarily happen where the data actually is located.
This is normally not a problem, but scales really badly and easily reaches physically limits of
the underlying hardware e.g. in networking and causes further headaches on any kind of failure.

To circumvent this, the programming paradigm of MapReduce breaks the processing task into
[line-through]#two# three basic phases:

- In the *mapping phase* small [Java][] programs called *mappers* ingest and convert data according to
internal logic into key-value pairs.
- This data is collected in the *shuffling phase*, sorted and similar data piped to a single matching
*reducers* if possible.
- And in the final *reducing phase* the *reducers* pick the sorted data up, aggregate and further
refine it and ultimately write it to a store like [HDFS][].

++++
{% plantuml %}
' !theme unexist from /home/unexist/blog/assets/plantuml
!theme unexist from ../assets/plantuml
skinparam linetype ortho
skinparam nodesep 30
skinparam ranksep 30

together {
{% raw %}
  file in1 [
{{json
  {
    "title":"string","description":"string","done":false,"dueDate":{"start":"2021-05-07","due":"2021-05-07"},"id":0
  }
}}
  ]

  file in2 [
{{json
  {
    "title":"string","description":"string","done":false,"dueDate":{"start":"2021-05-07","due":"2021-05-07"},"id":1
  }
}}
  ]
}
{% endraw %}

together {
  rectangle "Map (1)" as map1 #red
  rectangle "Map (1)" as map2 #red
}

together {
  file shuf1 [
2021-05-07 => 0
  ]

  file shuf2 [
2021-05-07 => 1
  ]
}

file out [
2021-06-07 => (0, 1)
]

rectangle "Shuffle (2)" as shuffle #green
rectangle "Reduce (3)" as reduce #blue

in1 -> map1
in2 -> map2

map1 -> shuf1
map2 -> shuf2

shuf1 -> shuffle
shuf2 -> shuffle

shuffle -> reduce

reduce -> out

' Layout

in1 -[hidden]-> in2
map1 -[hidden]-> map2
shuf1 -[hidden]-> shuf2
{% endplantuml %}
++++

<1> One mapper per file converts our JSON into a single key-value pair.
<2> The shuffle step ensures that similar pairs are fed into the same reducer.
<3> Finally the reducer compacts and combines the key-pairs.

=== Mapper

In the first step a mapper consumes the data supplied by the various
[RecordReader][] implementations of the framework.
There exists a broad range of different implementations of the well-known
formats like [Parquet][], [Avro][] and even flat data, so the processing here
is quite agnostic of it.

The interesting method here is *map*, which can be supplemented with optional *setup*
and *cleanup* steps to prepare and respectively tidy up the stage.

[source,java]
----
public class TodoMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    private Text dueDate = new Text();
    private ObjectMapper mapper = new ObjectMapper();

    protected void map(LongWritable key, Text value, Context context)
            throws java.io.IOException, InterruptedException
    {
        try {
            Todo todo = this.mapper.readValue(value.toString(), Todo.class); // <1>

            String formattedDate = todo.getDueDate().getDue()
                    .format(DateTimeFormatter.ofPattern(DueDate.DATE_PATTERN));

            context.write(new Text(formattedDate), new IntWritable(todo.getId())); // <2>
        } catch (JsonProcessingException e) {
            /* Do nothing */
        }
    }
}
----
<1> Parse the JSON-string and convert it to a Todo object.
<2> After that write the parsed date along with the id back to the context.

=== Partitioner and Combiner

During the shuffling phase [partitioners][] and [combiners][] implement the logic how the
data is distributed between the reducers.

If the partitioner is omitted, the framework just takes the hash of the keys, divides it the
number of possible reducers and splits it accordingly.
This normally guarantees equal distribution of data, but if there is any requirement to keep
specific data together a custom partitioner must be supplied.

A simple example might look like this:

[source,java]
----
public class TodoPartitioner extends Partitioner<Text, Text> {

    protected int getPartition(Text key, Text value, int numReduceTasks) {
        int id = Integer.parseInt(value);

        if (20 > id) {
            return 1 % numReduceTasks;
        } else {
            return 2 % numReduceTasks;
        }
    }
}
----

Combiners are quite similar to [reducers][] with the sole difference they are applied directly
*after* the mapping step and *before* the data is sent to the reducer on the same machine.
This allows to aggregate the data directly and there is a good chance that this reduces the data
that is sent across the network.

This is surely not an issue in this example, but it might have a huge impact in real scenarios.

=== Reducer

Analogous to a mapper, a reducer can have a *setup* and a *cleanup* step, but since it isn't
strictly required we can skip this here as well and jump to the *reduce* step where the actual
processing happens.

A reducer operates with some assumptions about the input data:

- They keys are sorted
- A single reducer is responsible for a specific key
- There can be a multitude of values in the supplied key-value pairs

[source,java]
----
public class TodoReducer extends Reducer<Text, IntWritable, Text, IntArrayWritable> {

    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws java.io.IOException,
            InterruptedException
    {
        List<Integer> idList = new ArrayList<>();

        for (IntWritable value : values) { // <1>
            idList.add(value.get());
        }

        context.write(key, new IntArrayWritable(idList.toArray(Integer[]::new))); // <2>
    }
}
----
<1> This simply collects all found ids and appends them to a list.
<2> When the data is written back to the context the [custom class][] `IntArrayWriteable` is used, which
has been omitted here for brevity.

== How to run it?

All processing happens with the help of jobs, which are submitted to and run on the data nodes holding
the actual data.

=== Creating a job

[source,java]
----
public class TodoCollect extends Configured implements Tool {

    public int run(String[] args) throws Exception {
        Path inputPath = new Path(args[0]);
        Path outputPath = new Path(args[1]);

        Configuration conf = new Configuration(true);

        Job job = Job.getInstance(conf, "TodoCollect"); // <1>

        job.setJarByClass(getClass());

        job.setMapperClass(TodoMapper.class); // <2>
        job.setReducerClass(TodoReducer.class);
        job.setNumReduceTasks(1);

        job.setOutputKeyClass(Text.class); // <3>
        job.setOutputValueClass(IntArrayWritable.class);

        FileInputFormat.addInputPath(job, inputPath); // <4>
        FileOutputFormat.setOutputPath(job, outputPath);

        return job.waitForCompletion(true) ? 0 : 1;
    }

    public static void main(String[] args) throws Exception {
        int exitCode = ToolRunner.run(new TodoCollect(), args);

        System.exit(exitCode);
    }
}
----
<1> The definition of a job is really straight forward.
<2> Mapper, reducer and any other steps like combiner can be configured here.
<3> The output types must be supplied in order to write the data back to storage.
<4> We supply the input and output path via argument.

=== Testing with MRUnit

[source,java]
----
public class TodoMapperReducerTest {
    final static String RECORD =
            "{\"title\":\"string\",\"description\":\"string\",\"done\":false,\"dueDate\":{\"start\":\"2021-05-07\",\"due\":\"2021-05-07\"},\"id\":0}";

    MapDriver<LongWritable, Text, Text, IntWritable> mapDriver;
    ReduceDriver<Text, IntWritable, Text, IntWritable> reduceDriver;
    MapReduceDriver<LongWritable, Text, Text, IntWritable, Text, IntWritable> mapReduceDriver;

    @Before
    public void setUp() {
        TodoMapper mapper = new TodoMapper();
        TodoReducer reducer = new TodoReducer();

        mapDriver = MapDriver.newMapDriver(mapper);
        reduceDriver = ReduceDriver.newReduceDriver(reducer);
        mapReduceDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer);
    }

    @Test
    public void shouldVerifyMapper() throws IOException {
        mapDriver.withInput(new LongWritable(), new Text(RECORD));
        mapDriver.withOutput(new Text("2021-05-07"), new IntWritable(1));
        mapDriver.runTest();
    }

    @Test
    public void shouldVerifyReducer() throws IOException {
        List<IntWritable> values = new ArrayList<IntWritable>();

        values.add(new IntWritable(1));
        values.add(new IntWritable(1));

        reduceDriver.withInput(new Text("2021-05-07"), values);
        reduceDriver.withOutput(new Text("2021-05-07"), new IntWritable(2));
        reduceDriver.runTest();
    }

    @Test
    public void shouldVerfiyMapAndReduce() throws IOException {
        mapReduceDriver.withInput(new LongWritable(), new Text(RECORD));

        List<IntWritable> values = new ArrayList<IntWritable>();

        values.add(new IntWritable(1));
        values.add(new IntWritable(1));

        mapReduceDriver.withOutput(new Text("2021-05-07"), new IntWritable(1));
        mapReduceDriver.runTest();
    }
}
----

== Conclusion

All examples can be found here:

<https://github.com/unexist/showcase-hadoop-cdc-quarkus/tree/master/todo-mapreduce>

[bibliography]
== Bibliography

* [[[hadooparch]]] Mark Grover, Ted Malask, Jonathan Seidman, Gwen Shapira, Hadoop Application Architectures, O'Reilly 2015