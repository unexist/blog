---
layout: post
title: MapReduce
date: 2023-09-02 16:05:48 +0200
last_updated: 2023-09-02 16:05:48 +0200
author: Christoph Kappel
tags: hadoop big-data mapreduce showcase
categories: tech
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/mapreduce
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/mapreduce
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

////
https://mrunit.apache.org/
////

In the previous post of my [Big Data][] series we started with a bit of the technical concepts of
[Hadoop][], briefly covered its computing model and explored how it can be used as a plain file
storage.
This is from a technical point interesting, but storage alone is only half the fun.

So in this follow-up we are going to dive into the world of [MapReduce][], the unit-test framework
[MRUnit][] and a really simple [todo][] example to demonstrate the advantages of the whole idea.

While not strictly required to understand why Hadoop is an excellent choice for storage, it really
helps to have a bit of understanding for the course of this article, so if you haven't read the
previous post yet, we will wait - promised:

<https://blog.unexist.dev/big-data/2023/10/27/big-data.html>

Ready? So without further ado let us start.

== What is MapReduce?

One of the major problems with processing large quantities of data is that the processing of it
doesn't necessarily happen where the data actually is located.
This is normally not a problem, but scales really badly and easily reaches physically limits of
the underlying hardware e.g. in networking and causes further headaches on any kind of failure.

To circumvent this, the programming paradigm of MapReduce breaks the processing task into
[line-through]#two# three basic phases:

- In the *mapping phase* small [Java][] programs called *mappers* ingest and convert data according to
internal logic into key-value pairs.
- This data is collected in the *shuffling phase*, sorted and similar data piped to a single matching
*reducers* if possible.
- And in the final *reducing phase* the *reducers* pick the sorted data up, aggregate and further
refine it and ultimately write it to a store like [HDFS][].

++++
{% plantuml %}
' !theme unexist from /home/unexist/blog/assets/plantuml
!theme unexist from ../assets/plantuml
skinparam linetype ortho
skinparam nodesep 30
skinparam ranksep 30

together {
{% raw %}
  file in1 [
{{json
  {
    "title":"string","description":"string","done":false,"dueDate":{"start":"2021-05-07","due":"2021-05-07"},"id":0
  }
}}
  ]

  file in2 [
{{json
  {
    "title":"string","description":"string","done":false,"dueDate":{"start":"2021-05-07","due":"2021-05-07"},"id":1
  }
}}
  ]
}
{% endraw %}

together {
  rectangle "Map (1)" as map1 #red
  rectangle "Map (1)" as map2 #red
}

together {
  file shuf1 [
2021-05-07 => 0
  ]

  file shuf2 [
2021-05-07 => 1
  ]
}

file out [
2021-06-07 => (0, 1, 2)
]

rectangle "Shuffle (2)" as shuffle #green
rectangle "Reduce (3)" as reduce #blue

in1 -> map1
in2 -> map2

map1 -> shuf1
map2 -> shuf2

shuf1 -> shuffle
shuf2 -> shuffle

shuffle -> reduce

reduce -> out

' Layout

in1 -[hidden]-> in2
map1 -[hidden]-> map2
shuf1 -[hidden]-> shuf2
{% endplantuml %}
++++

<1> One mapper per file converts our JSON into a single key-value pair.
<2> During shuffle the framework ensures similar pairs are fed into the same reducer.
<3> Finally the reducer compacts and combines the key-pairs.

=== Mapper

[source,java]
----
public class TodoMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    private Text dueDate = new Text();
    private ObjectMapper mapper = new ObjectMapper();

    protected void map(LongWritable key, Text value, Context context)
            throws java.io.IOException, InterruptedException
    {
        try {
            Todo todo = this.mapper.readValue(value.toString(), Todo.class);

            String formattedDate = todo.getDueDate().getDue()
                    .format(DateTimeFormatter.ofPattern(DueDate.DATE_PATTERN));

            context.write(new Text(formattedDate), new IntWritable(todo.getId()));
        } catch (JsonProcessingException e) {
            /* Do nothing */
        }
    }
}
----

=== Partitioner and Combiner

During the shuffling phase [partitioners][] and [combiners][] implement the logic how the data is
distributed between the reducers.

If the partitioner is omitted, the framework just takes the hash of the keys, divides it the
number of possible reducers and splits it accordingly.
This normally guarantees equal distribution of data, but if there is any requirement to keep
specific data together a custom partitioner must be supplied.

A simple example might look like this:

[source,java]
----
public class TodoPartitioner extends Partitioner<Text, Text> {

    protected int getPartition(Text key, Text value, int numReduceTasks) {
        int id = Integer.parseInt(value);

        if (20 > id) {
            return 1 % numReduceTasks;
        } else {
            return 2 % numReduceTasks;
        }
    }
}
----

Combiners are quite similar to [reducers][] with the sole difference they are applied directly
after the mapping and before the data is sent to the reducer on the same machine.
This allows to aggregate the data directly and there is a good chance that this reduces the data
that is sent across the network.

This is surely not an issue in this example, but it might have a huge impact in real scenarios.

=== Reducer

[source,java]
----
public class TodoReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws java.io.IOException,
            InterruptedException
    {
        int sum = 0;

        for (IntWritable value : values) {
            sum += value.get();
        }

        context.write(key, new IntWritable(sum));
    }
}
----

== Testing with MRUnit




[source,java]
----
public class TodoMapperReducerTest {
    final static String RECORD =
            "{\"title\":\"string\",\"description\":\"string\",\"done\":false,\"dueDate\":{\"start\":\"2021-05-07\",\"due\":\"2021-05-07\"},\"id\":0}";

    MapDriver<LongWritable, Text, Text, IntWritable> mapDriver;
    ReduceDriver<Text, IntWritable, Text, IntWritable> reduceDriver;
    MapReduceDriver<LongWritable, Text, Text, IntWritable, Text, IntWritable> mapReduceDriver;

    @Before
    public void setUp() {
        TodoMapper mapper = new TodoMapper();
        TodoReducer reducer = new TodoReducer();

        mapDriver = MapDriver.newMapDriver(mapper);
        reduceDriver = ReduceDriver.newReduceDriver(reducer);
        mapReduceDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer);
    }

    @Test
    public void shouldVerifyMapper() throws IOException {
        mapDriver.withInput(new LongWritable(), new Text(RECORD));
        mapDriver.withOutput(new Text("2021-05-07"), new IntWritable(1));
        mapDriver.runTest();
    }

    @Test
    public void shouldVerifyReducer() throws IOException {
        List<IntWritable> values = new ArrayList<IntWritable>();

        values.add(new IntWritable(1));
        values.add(new IntWritable(1));

        reduceDriver.withInput(new Text("2021-05-07"), values);
        reduceDriver.withOutput(new Text("2021-05-07"), new IntWritable(2));
        reduceDriver.runTest();
    }

    @Test
    public void shouldVerfiyMapAndReduce() throws IOException {
        mapReduceDriver.withInput(new LongWritable(), new Text(RECORD));

        List<IntWritable> values = new ArrayList<IntWritable>();

        values.add(new IntWritable(1));
        values.add(new IntWritable(1));

        mapReduceDriver.withOutput(new Text("2021-05-07"), new IntWritable(1));
        mapReduceDriver.runTest();
    }
}
----

== Conclusion

All examples can be found here:

<https://github.com/unexist/showcase-hadoop-cdc-quarkus/>

[bibliography]
== Bibliography

* [[[hadooparch]]] Mark Grover, Ted Malask, Jonathan Seidman, Gwen Shapira, Hadoop Application Architectures, O'Reilly 2015