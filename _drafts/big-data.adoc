---
layout: post
title: Big Data
date: %%%DATE%%%
last_updated: %%%DATE%%%
author: Christoph Kappel
tags: showcase
categories: showcase hadoop hive iceberg debezium
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/big_data
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/big_data
endif::[]
:figure-caption!:
:table-caption!:

```
https://github.com/unexist/showcase-hadoop-cdc-quarkus/blob/master/podman/hadoop_hive_spark/Dockerfile
https://www.linkedin.com/pulse/musl-libc-alpines-greatest-weakness-rogan-lynch/
https://wiki.musl-libc.org/functional-differences-from-glibc.html
https://aws.amazon.com/compare/the-difference-between-a-data-warehouse-data-lake-and-data-mart/
```

[Big Data] was just another buzzword for me and I never had any real contact besides the typical
requests of data scientists to move data to a specific endpoint in the overall architecture.
This changed, when I got the opportunity to have a closer look at a big data project at work.

I don't like to deal with tech stuff unprepared, so filled in some blanks by reading some articles,
a few books (<<datalake>>, <<practical>>, <<bigdata>> and <<hadoop>>) and checked out some
pre-constructed containers.
Some of these containers actually [line-through]#worked# started, but I knew I had to ultimately
roll my own version to understand the intricate design and communication between all the moving
parts.

So during the course of this post we are going to cover the brief idea, fire up my own version
of a single-container-data lake and use some complex ways move some data into it from the usual
[demo application][].

== What is Big Data?

Historically I'd say the introduction of big data generally marks the shift away from storing
processed or in some ways refined data to just storing anything as is - just in case it might
become useful later.

Storing everything piles up pretty quickly and gets even worse when you actually work with it:

[link=https://xkcd.com/2582/]
.Source <https://xkcd.com/2582/>
image::data_trap.png[]

=== Reducing complexity

There have been several iterations of tackling the overall complexity on the storage level.
Like all good stories it started with SQL - there were [Data Warehouses] with big databases,
which kept everything in rigid schemas.
Unfortunately this introduced complex requirements for scaling and made redundancy even worse, but
did the job for a while.

They were followed by [Data Lakes] and [Data Marts], which do two main things differently:

- They contain data split by domain of origin or use-case
- No data schema is enforced on write and everything is flat (most likely flat-file)

In both cases the probably best underlying storage is [Hadoop], because it allows to offload some
tasks like redundancy and scaling to it.

=== Benefits of Hadoop

== Setting everything up

Although the [Dockerfile][] is quite huge, there isn't much complexity involved, but I still would
like to dedicate some lines to some issues I especially had fun with.

=== Alpine and Musl

The first bigger issues was the incompatibility of the C-based parts of [datanodes][] with the
default [libc][] implementation of [Alpine][].
[Musl][] is an excellent choice due to its small storage footprint for containers, but is not
without problems.

[quote,'https://martinheinz.dev/blog/92']
By using Alpine, you're getting "free" chaos engineering for your cluster.

There are some differences which can to lead to surprising issues and it is never a bad to idea to
check [this curated list][] for more information about it.

The best option for me here was to switch to another base image entirely:

[source,docker]
----
FROM eclipse-temurin:8-jdk
----

=== Podman capabilities

Coming from [ArchLinux][], I obviously still have the **rolling-update-way** deeply ingrained in
me and usually just hit update whenever I see such a message and also fire-up [brew][] and [rustup][]
quite frequently.

This usually works well - until it doesn't.

After updating my local [Podman][] (somehwere around `4.4.1`) installation I discovered I cannot
start [sshd][] inside of my container anymore.
Podman doesn't necessarily need sshd, but [Hadoop][] and friends rely heavily on it for
inter-node-communication:

[source,log]
----
ssh: Connection closed by 127.0.0.1 port 22
sshd: chroot("/run/sshd"): Operation not permitted [preauth]
----

I'd like to say this error directly pointed me to the changelog of Podman, but unfortunately I spent
some hours debugging and desperating until I finally found a hint on [Stack Overflow][] to have a
look at the release notes:

[link=https://github.com/containers/podman/blob/main/RELEASE_NOTES.md]
.Source: https://github.com/containers/podman/blob/main/RELEASE_NOTES.md
image::podman_capabilities.png[]

== Debezium, the WAL and an Iceberg

== Scala with a Spark

[bibliography]
== References

* [[datalake]] Alex Gorelik, The Enterprise Big Data Lake: Delivering the Promise of Big Data and Data Science, O'Reilly 2019
* [[practical]] Saurabh Gupta, Practical Enterprise Data Lake Insights: Handle Data-Driven Challenges in an Enterprise Big Data Lake, Apress 2018
* [[bigdata]] Nathan Marz, Big Data, Manning 2019
* [[hadoop]] Tom White, Hadoop: The Definitive Guide, O'Reilly 2009
