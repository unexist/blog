---
layout: post
title: Big Data
date: %%%DATE%%%
last_updated: %%%DATE%%%
author: Christoph Kappel
tags: showcase
categories: showcase hadoop hive iceberg debezium
toc: true
---
:imagesdir: /assets/images/big_data
:figure-caption!:
:table-caption!:

```
https://github.com/unexist/showcase-hadoop-cdc-quarkus/blob/master/podman/hadoop_hive_spark/Dockerfile
https://www.linkedin.com/pulse/musl-libc-alpines-greatest-weakness-rogan-lynch/
https://wiki.musl-libc.org/functional-differences-from-glibc.html
```

I never had much contact with [Big Data][]-related topics before and as far as I was concerned
it exists and are used to put information there to rest.

This changed, when I had to fill in some blanks in preparation for a meeting for a possible
new project.
I've read some books about the overall idea (<<datalake>>, <<practical>> and <<bigdata>>), followed
blog posts and was pretty much enticed.
I knew there are some pre-constructed containers available for first steps, but I usually wanted
to set it up myself and also try to get my own data into it.

So in this blog post we are going to start with a brief introduction of the whole idea, fire up a
single-container data lake and play with two ways of putting data from our usual
[demo application][] into it.

== What is Big Data?

Historically I'd say the introduction of big data generally marks the shift away from storing
processed or in some ways refined data to just storing anything as is - just in case it might
become useful later.
This splendidly solves the problem of a pre-processing bottleneck in influx, but postpones the
problem to a later reader of the data - whoever tries to make sense of it.

This got out of hand quickly and the huge piles couldn't be handled anymore, so the big data idea
was revised and broken down into all-purpose data warehouses with a mix of all available domains,
a data lake with a single domain and even finer grained in a data mart.




Handling a huge amount of data comes with problems of its own right.
The first obvious one is it takes a lot of storage space and


One of the major problems of today's data-based systems is the huge complexity, which is required to
address modern expectations of fault-tolerance and obviously the performance or rather latency of
the systems.

This can be easy for smaller scales, but causes lots of headaches in the petabytes range of data.

Unfortunately the storage size is only one-half of the problem, processing large quantities of data
also dramatically increases the processing time up to the point where a timely response cannot be
guaranteed anymore without cutting some corners.

Entering big data!

=== Reducing complexity

The easiest way of reducing complexity is to either avoid it or just blame someone else for it.
And somehow this is the best approach


[link=https://xkcd.com/2582/]
.Source <https://xkcd.com/2582/>
image::data_trap.png[]

== Setting everything up

Although the [Dockerfile][] is quite huge, there isn't much complexity involved, but I still would
like to dedicate some lines to some issues I especially had fun with.

=== Alpine and Musl

The first bigger issues was the incompatibility of the C-based parts of [datanodes][] with the
default [libc][] implementation of [Alpine][].
[Musl][] is an excellent choice due to its small storage footprint for containers, but is not
without problems.

[quote,'https://martinheinz.dev/blog/92']
By using Alpine, you're getting "free" chaos engineering for your cluster.

There are some differences which can to lead to surprising issues and it is never a bad to idea to
check [this curated list][] for more information about it.

The best option for me here was to switch to another base image entirely:

[source,docker]
----
FROM eclipse-temurin:8-jdk
----

=== Podman capabilities

Coming from [ArchLinux][], I obviously still have the **rolling-update-way** deeply ingrained in
me and usually just hit update whenever I see such a message and also fire-up [brew][] and [rustup][]
quite frequently.

This usually works well - until it doesn't.

After updating my local [Podman][] (somehwere around `4.4.1`) installation I discovered I cannot
start [sshd][] inside of my container anymore.
Podman doesn't necessarily need sshd, but [Hadoop][] and friends rely heavily on it for
inter-node-communication:

[source,log]
----
ssh: Connection closed by 127.0.0.1 port 22
sshd: chroot("/run/sshd"): Operation not permitted [preauth]
----

I'd like to say this error directly pointed me to the changelog of Podman, but unfortunately I spent
some hours debugging and desperating until I finally found a hint on [Stack Overflow][] to have a
look at the release notes:

[link=https://github.com/containers/podman/blob/main/RELEASE_NOTES.md]
.Source: https://github.com/containers/podman/blob/main/RELEASE_NOTES.md
image::podman_capabilities.png[]

== Debezium, the WAL and an Iceberg

== Scala with a Spark

[bibliography]
== References

* [[datalake]] Alex Gorelik, The Enterprise Big Data Lake: Delivering the Promise of Big Data and Data Science, O'Reilly 2019
* [[practical]] Saurabh Gupta, Practical Enterprise Data Lake Insights: Handle Data-Driven Challenges in an Enterprise Big Data Lake, Apress 2018
* [[bigdata]] Nathan Marz, Big Data, Manning 2019
