---
layout: post
title: Big Data
date: %%%DATE%%%
last_updated: %%%DATE%%%
author: Christoph Kappel
tags: showcase
categories: showcase hadoop hive iceberg debezium
toc: true
---
:imagesdir: /assets/images/big_data
:figure-caption!:
:table-caption!:

```
https://github.com/unexist/showcase-hadoop-cdc-quarkus/blob/master/podman/hadoop_hive_spark/Dockerfile
https://www.linkedin.com/pulse/musl-libc-alpines-greatest-weakness-rogan-lynch/
https://wiki.musl-libc.org/functional-differences-from-glibc.html
```

I never had much contact with the general topic of [Big Data][] or even [Data Lakes][] before and
I mainly knew both just exists and is a hype.
And recently I had to fill in some blanks in preparation for a meeting for a possible new project.

After reading some books and blog posts about the overall idea I pretty much wanted to start
playing with it.
Setting everything up was a challenge by itself, but the more difficult one was how to get actual
data there in the first place?

So in this blog post we are going to start with a brief introduction, set up a single-container data
lake and play with two ways of moving data from our usual [demo application][] into it.

== What is Big Data?

One of the major problems of todays data-based systems is the huge complexity, that is required to
address modern concerns for to fault-tolerancy and obviously the latency of the systems.


== Setting everything up

Although the [Dockerfile][] is quite huge, there isn't much complexity involved, but I still would
like to dedicate some lines to some issues I especially had fun with.

=== Alpine and Musl

The first bigger issues was the incompatibility of the C-based parts of [datanodes][] with the
default [libc][] implementation of [Alpine][].
[Musl][] is an excellent choice due to its small storage footprint for containers, but is not
without problems.

[quote,'https://martinheinz.dev/blog/92']
By using Alpine, you're getting "free" chaos engineering for your cluster.

There are some differences which can to lead to surprising issues and it is never a bad to idea to
check [this curated list][] for more information about it.

The best option for me here was to switch to another base image entirely:

[source,docker]
----
FROM eclipse-temurin:8-jdk
----


=== Podman capabilities

Coming from [ArchLinux][], I obviously still have the **rolling-update-way** deeply ingrained in
me and usually just hit update whenever I see such a message and also fire-up [brew][] and [rustup][]
quite frequently.

This usually works well - until it doesn't.

After updating my local [Podman][] (somehwere around `4.4.1`) installation I discovered I cannot
start [sshd][] inside of my container anymore.
Podman doesn't necessarily need sshd, but [Hadoop][] and friends rely heavily on it for
inter-node-communication:

[source,log]
----
ssh: Connection closed by 127.0.0.1 port 22
sshd: chroot("/run/sshd"): Operation not permitted [preauth]
----

I'd like to say this error directly pointed me to the changelog of Podman, but unfortunately I spent
some hours debugging and desperating until I finally found a hint on [Stack Overflow][] to have a
look at the release notes:

[link=https://github.com/containers/podman/blob/main/RELEASE_NOTES.md]
.Source: https://github.com/containers/podman/blob/main/RELEASE_NOTES.md
image::podman_capabilities.png[]

== Debezium, the WAL and an Iceberg

== Scala with a Spark