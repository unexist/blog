---
layout: post
title: Big Data
date: %%%DATE%%%
last_updated: %%%DATE%%%
author: Christoph Kappel
tags: showcase
categories: showcase hadoop hive iceberg debezium
toc: true
---
:imagesdir: /assets/images/big_data
:figure-caption!:
:table-caption!:

```
https://github.com/unexist/showcase-hadoop-cdc-quarkus/blob/master/podman/hadoop_hive_spark/Dockerfile
https://www.linkedin.com/pulse/musl-libc-alpines-greatest-weakness-rogan-lynch/
```

Looking into something more deeply usually has the nice benefit of completely changing your own
mind about it.
And this can ultimately promote something from a mere [buzzword][] to a something nice with
interesting technical implications.

I never had much contact with the general topic of [Data Lakes][] before, but had to fill in some
blanks in preparation for a meeting with a prospect.
After reading some books and blog posts about the overall idea I pretty much wanted to start
playing with it.
Setting everything up was a challenge by itself, but the more difficult one is how does the data
gets there in the first place?

So in this blog post we are going to set up a single-container data-lake and play with two ways of
moving data from our usual [demo application][] into it.

== Setting everything up

Although the [Dockerfile][] is quite huge, there isn't much complexity involved and I'd like to
use this paragraph to talk about some fun issues I had with it.

=== Alpine and Musl

The first bigger issues that I faced is the incompatibility of the C-based parts of the datanodes
with the default [libc][] of [Alpine][].
Instead of the more common [glibc][] it relies on [Musl][] due to its smaller footprint in storage.

=== Podman removed capabilities

image::podman_capabilites[]

== Debezium, the WAL and an Iceberg

== Scala with a Spark