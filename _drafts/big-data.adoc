---
layout: post
title: Big Data
date: %%%DATE%%%
last_updated: %%%DATE%%%
author: Christoph Kappel
tags: showcase
categories: showcase hadoop hive iceberg debezium
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/big_data
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/big_data
endif::[]
:figure-caption!:
:table-caption!:

```
https://github.com/unexist/showcase-hadoop-cdc-quarkus/blob/master/podman/hadoop_hive_spark/Dockerfile
https://www.linkedin.com/pulse/musl-libc-alpines-greatest-weakness-rogan-lynch/
https://wiki.musl-libc.org/functional-differences-from-glibc.html
https://aws.amazon.com/compare/the-difference-between-a-data-warehouse-data-lake-and-data-mart/
```

[Big Data] was just another buzzword for me and I never had any real contact besides a bit of
proximity due to the overall architecture - there are industries out there that like to keep
everything.

Then I got the opportunity to have a closer look at it during my preparations for a possible
upcoming project on my quest to fill in some blanks at least.
I've read a few books about the overall idea (<<datalake>>, <<practical>>, <<bigdata>> and
<<hadoop>>), followed by lots of blog posts, articles and tweets and was suddenly pretty much
enticed.

There are some pre-constructed containers available for a starter, but since I usually want to
get my own hand dirty you are invited to accompany me.

So during the course of this post we are going to cover the brief idea, fire up my own version
of a single-container-data lake and use some complex ways move some data into it from the usual
[demo application][].

== What is Big Data?

Historically I'd say the introduction of big data generally marks the shift away from storing
processed or in some ways refined data to just storing anything as is - just in case it might
become useful later.

Storing everything piles up pretty quickly and gets even worse when you actually work with it:

[link=https://xkcd.com/2582/]
.Source <https://xkcd.com/2582/>
image::data_trap.png[]

One of the first ideas was to store everything in a [Data Warehouse], which basically consists of
vast SQL-databases and enforces the same rigid schema for everything.
This worked for some time, but also introduced complex requirements for scaling and makes redundancy
even worse.

Splitting this data by domain of origin eased the operative pain a bit, but the removing the
requirement of a schema at all really made the handling easier in [Data Lakes].


That was broken apart into [Data Lakes][] for schemaless data of single domains, which basically
offloaded the refinement step to a future reader and was even more broken down into
[Data Marts][] for more specific use cases.

=== Reducing complexity

The easiest way of reducing complexity is to either avoid it or just blame someone else for it.

== Setting everything up

Although the [Dockerfile][] is quite huge, there isn't much complexity involved, but I still would
like to dedicate some lines to some issues I especially had fun with.

=== Alpine and Musl

The first bigger issues was the incompatibility of the C-based parts of [datanodes][] with the
default [libc][] implementation of [Alpine][].
[Musl][] is an excellent choice due to its small storage footprint for containers, but is not
without problems.

[quote,'https://martinheinz.dev/blog/92']
By using Alpine, you're getting "free" chaos engineering for your cluster.

There are some differences which can to lead to surprising issues and it is never a bad to idea to
check [this curated list][] for more information about it.

The best option for me here was to switch to another base image entirely:

[source,docker]
----
FROM eclipse-temurin:8-jdk
----

=== Podman capabilities

Coming from [ArchLinux][], I obviously still have the **rolling-update-way** deeply ingrained in
me and usually just hit update whenever I see such a message and also fire-up [brew][] and [rustup][]
quite frequently.

This usually works well - until it doesn't.

After updating my local [Podman][] (somehwere around `4.4.1`) installation I discovered I cannot
start [sshd][] inside of my container anymore.
Podman doesn't necessarily need sshd, but [Hadoop][] and friends rely heavily on it for
inter-node-communication:

[source,log]
----
ssh: Connection closed by 127.0.0.1 port 22
sshd: chroot("/run/sshd"): Operation not permitted [preauth]
----

I'd like to say this error directly pointed me to the changelog of Podman, but unfortunately I spent
some hours debugging and desperating until I finally found a hint on [Stack Overflow][] to have a
look at the release notes:

[link=https://github.com/containers/podman/blob/main/RELEASE_NOTES.md]
.Source: https://github.com/containers/podman/blob/main/RELEASE_NOTES.md
image::podman_capabilities.png[]

== Debezium, the WAL and an Iceberg

== Scala with a Spark

[bibliography]
== References

* [[datalake]] Alex Gorelik, The Enterprise Big Data Lake: Delivering the Promise of Big Data and Data Science, O'Reilly 2019
* [[practical]] Saurabh Gupta, Practical Enterprise Data Lake Insights: Handle Data-Driven Challenges in an Enterprise Big Data Lake, Apress 2018
* [[bigdata]] Nathan Marz, Big Data, Manning 2019
* [[hadoop]] Tom White, Hadoop: The Definitive Guide, O'Reilly 2009
