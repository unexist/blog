---
layout: post
title: Big Data
#date: %%%DATE%%%
#last_updated: %%%DATE%%%
author: Christoph Kappel
tags: hadoop big-data showcase
categories: big-data
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/big_data
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/big_data
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

////
https://github.com/unexist/showcase-hadoop-cdc-quarkus/
https://aws.amazon.com/compare/the-difference-between-a-data-warehouse-data-lake-and-data-mart/
https://cloud.google.com/learn/what-is-hadoop
https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html
////

[Big Data] was just another buzzword for me and I never had any real contact besides the typical
requests of data scientists to move data to a specific endpoint of a [Data Lake] in the overall
architecture.

This changed, when I got the opportunity to have a closer look at a big data project in a business
context.
I usually don't like to deal with new tech stuff unprepared, so I filled in some blanks by reading
articles, a few books (<<datalake>>, <<practical>>, <<bigdata>> and <<hadoop>>) and just started
playing with it.

This led to lots of insights and Aha!-moments and I really wanted to write about, but unfortunately
not only the name is big.
Condensing all the stuff I'd like to address is quite difficult, so this is the start of a small
series about the groundwork and (at least to me) interesting tools and aspects.

== What is Big Data?

From my impression the biggest thing in the big data idea is the shift away from storing processed
or in some ways refined data to just storing anything as is - just in case it might become useful
later.

This initially sounds like a good idea, but storing everything piles up pretty quickly and gets
even worse when you actually work with it:

[link=https://xkcd.com/2582/]
.Source <https://xkcd.com/2582/>
image::data_trap.png[]

=== Managing complexity

There have been several iterations of tackling the overall complexity on the storage level and like
all good stories it started with SQL.

Once upon a time, there were [Data Warehouses] with big databases, which kept everything in rigid
schemas.
Unfortunately this introduced complex requirements for scaling and made redundancy even worse, but
did the job for a while and everyone was happy...

Later on they were augmented or replaced by [Data Lakes] and [Data Marts], which did some things
differently:

- Data is split by domain of origin or simply use-case
- No schema is enforced on write and everything stays as-is in mostly flat-file
- The problem of handling the actual data is offloaded to [Hadoop]

== What is Hadoop?

Hadoop is an open-source [Java] framework that manages large quantities of data and provides means
for further processing.
We are going to cover the processing of the actual data with [MapReduce] in a follow-up article,
but before we can do that we have to talk about the architecture and the major components first:

- *Hadoop Distributed File System (HDFS)* - is the underlying distributed filesystem and allows
the Hadoop nodes to operate on local data.
- *Yet Another Resource Negotiator (YARN)* - is the resource-management part of the system and
responsible for scheduling jobs and resource allocation.
- *MapReduce* - is a programming model for the actual processing in a [divide-and-conquer]-way
and is going to be covered in a further post.
- *Hadoop Common* - is a collection of all supporting tools and libraries.

=== Architecture

The core architecture of HDFS is based on a leader/follower principle with a strong focus on
resiliency and redundancy.

++++
{% plantuml %}
!theme unexist from ../assets/plantuml
skinparam linetype ortho

together {
  package "Namenode" as nn {
    card Metadata [
    Metadata
    ====
    Name
    ----
    Replicas
    ----
    Path
    ]
  }

  package "Datanodes" as dns {
    card "Datanode" as dn1 {
      file "File" as f1
      file "File" as f2
    }

    card "Datanode" as dn2 {
      file "File" as f3
      file "File" as f4
    }

    card "Datanode" as dn3 {
      file "File" as f5
      file "File" as f6
    }
  }
}

usecase "Client" as c

nn -[hidden]-> dns
'dns -[hidden]-> c

c --> nn : Metadata ops (1)
c <-- f1 : Read (2)
c --> f3 : Write (3)

nn --> dns : Block ops (4)

@enduml
{% endplantuml %}
++++

<1> Namesnodes keep track of every block and store metadata like the name or the count of active
replicas and answer questions of clients which datanodes have a copy of the desired block.
<2> Clients use this information to connect directly to the datanodes and load the actual data.
<3> When a client wants to write data it again consults the namesnodes first and then writes the data
to the datanodes.
<4> And namenodes check if the data needs to be re-balanced or replicated based on different events
like a datanode

=== Files

++++
{% plantuml %}
!theme unexist from ../assets/plantuml
left to right direction

database "Catalog Layer" {
  [Catalog]
}

node "Metadata Layer" {
  [Metadata File]
  [Manifest List]
  [Manifest File]
}


node "Data Layer" {
  [Data Files]
}

[Catalog] --> [Metadata File]
[Metadata File] --> [Manifest List]
[Manifest List] --> [Manifest File]
[Manifest File] --> [Data Files]
{% endplantuml %}
++++

=== Benefits

=== How to use the API?

==== Setup

[source,java]
----
Configuration configuration = new Configuration();

String baseDir = Files.createTempDirectory("test_hdfs").toFile().getAbsoluteFile();

configuration.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, baseDir.getAbsolutePath());

MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(configuration);

MiniDFSCluster cluster = builder.build();
----

==== Write data

[source,java]
----
FileSystem fileSystem = FileSystem.get(configuration);

Path hdfsPath = new Path(HADOOP_FILE);
FSDataOutputStream fsOut;

if (fileSystem.exists(hdfsPath)) {
    fsOut = fileSystem.append(hdfsPath);
} else {
    fsOut = fileSystem.create(hdfsPath);
}

OutputStreamWriter outStreamWriter = new OutputStreamWriter(fsOut, StandardCharsets.UTF_8);

BufferedWriter bufferedWriter = new BufferedWriter(outStreamWriter);

bufferedWriter.write("data");

bufferedWriter.close();
outStreamWriter.close();
fsOut.close();
fileSystem.close()
----

==== Read data

[source,java]
----
FileSystem fileSystem = FileSystem.get(configuration);

Path hdfsPath = new Path(HADOOP_FILE);

FSDataInputStream inputStream = fileSystem.open(hdfsPath);

BufferedReader bufferedReader = new BufferedReader(
    new InputStreamReader(inputStream, StandardCharsets.UTF_8));

String line = null;
while (null != (line = bufferedReader.readLine())) {
    LOGGER.debug("Read line: %s", line);
}

inputStream.close();
fileSystem.close();
----

[bibliography]
== References

* [[[datalake]]] Alex Gorelik, The Enterprise Big Data Lake: Delivering the Promise of Big Data and Data Science, O'Reilly 2019
* [[[practical]]] Saurabh Gupta, Practical Enterprise Data Lake Insights: Handle Data-Driven Challenges in an Enterprise Big Data Lake, Apress 2018
* [[[bigdata]]] Nathan Marz, Big Data, Manning 2019
* [[[hadoop]]] Tom White, Hadoop: The Definitive Guide, O'Reilly 2009
