---
layout: post
title: Big Data
date: %%%DATE%%%
last_updated: %%%DATE%%%
author: Christoph Kappel
tags: showcase
categories: showcase hadoop hive iceberg debezium
toc: true
---
:imagesdir: /assets/images/big_data
:figure-caption!:
:table-caption!:

```
https://github.com/unexist/showcase-hadoop-cdc-quarkus/blob/master/podman/hadoop_hive_spark/Dockerfile
https://www.linkedin.com/pulse/musl-libc-alpines-greatest-weakness-rogan-lynch/
https://wiki.musl-libc.org/functional-differences-from-glibc.html
```

I never had much contact with the general topic of [Big Data][] or even [Data Lakes][] before and
I mainly knew both just exists and is a hype.
And recently I had to fill in some blanks in preparation for a meeting for a possible new project.

After reading some books and blog posts about the overall idea I pretty much wanted to start
playing with it.
Setting everything up was a challenge by itself, but the more difficult one was how to get actual
data there in the first place?

So in this blog post we are going to start with a brief introduction, set up a single-container data
lake and play with two ways of moving data from our usual [demo application][] into it.

== What is Big Data?

== Setting everything up

Although the [Dockerfile][] is quite huge, there isn't much complexity involved and I'd like to
use this paragraph to talk about some fun issues I had to deal with.

=== Alpine and Musl

The first bigger issues was the incompatibility of the C-based parts of [datanodes][] with the
default [libc][] implementation of [Alpine][].
[Musl][] is an excellent choice due to its small storage footprint for containers, but is not
without problems.
There are some differences which can to lead to surprising issues and it is never a bad to idea to
check [this curated list][] for more information about it.

The best option for me here was to switch to another base image.

[quote,'https://martinheinz.dev/blog/92']
By using Alpine, you're getting "free" chaos engineering for your cluster.

=== Podman capabilities

One of the drawbacks of using software in a **rolling-update-way** is that you run head first into
every breaking change, unless you make it a good habit of checking changelogs before doing the
actual update.
Needless to say I didn't do that and discovered a nice change introduced in [Podman][] `4.4.1`.

After updating my local Podman installation I couldn't start [sshd][] inside of my container
anymore and [Hadoop][] and friends rely heavily on it for inter-communication:

[source,log]
----
ssh: Connection closed by 127.0.0.1 port 22
sshd: chroot("/run/sshd"): Operation not permitted [preauth]
----

I'd like to say this error directly pointed me to the changelog of Podman, but unfortunately I spent
some hours debugging and finally found a hint on [Stack Overflow][] to have a look here:

[link=https://github.com/containers/podman/blob/main/RELEASE_NOTES.md]
.Source: https://github.com/containers/podman/blob/main/RELEASE_NOTES.md
image::podman_capabilities.png[]

== Debezium, the WAL and an Iceberg

== Scala with a Spark