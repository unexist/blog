---
layout: post
title: Big Data
#date: %%%DATE%%%
#last_updated: %%%DATE%%%
author: Christoph Kappel
tags: hadoop big-data showcase
categories: big-data
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/big_data
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/big_data
endif::[]
:figure-caption!:
:table-caption!:

```
https://github.com/unexist/showcase-hadoop-cdc-quarkus/
https://aws.amazon.com/compare/the-difference-between-a-data-warehouse-data-lake-and-data-mart/
```

[Big Data] was just another buzzword for me and I never had any real contact besides the typical
requests of data scientists to move data to a specific endpoint of a [Data Lake] in the overall
architecture.

This changed, when I got the opportunity to have a closer look at a big data project in a business
context.
I usually don't like to deal with new tech stuff unprepared, so I filled in some blanks by reading
articles, a few books (<<datalake>>, <<practical>>, <<bigdata>> and <<hadoop>>) and basically
started playing with it.

This led to lots of insights and Aha!-moments I really want to write about, but unfortunately not
only the name is big.
Condensing all the stuff I'd like to address is quite difficult, so this is the start of a small
series about the groundwork and (at least to me) interesting aspects and technologies.

== What is Big Data?

Historically I'd say the introduction of big data generally marks the shift away from storing
processed or in some ways refined data to just storing anything as is - just in case it might
become useful later.

Storing everything piles up pretty quickly and gets even worse when you actually work with it:

=== Reducing complexity

There have been several iterations of tackling the overall complexity on the storage level.
Like all good stories it started with SQL - there were [Data Warehouses] with big databases,
which kept everything in rigid schemas.
Unfortunately this introduced complex requirements for scaling and made redundancy even worse, but
did the job for a while.

They were followed by [Data Lakes] and [Data Marts], which do two main things differently:

- They contain data split by domain of origin or use-case
- No data schema is enforced on write and everything is flat (most likely flat-file)

In both cases the probably best underlying storage is [Hadoop], because it allows to offload some
tasks like redundancy and scaling to it.

=== Benefits of Hadoop

[bibliography]
== References

* [[datalake]] Alex Gorelik, The Enterprise Big Data Lake: Delivering the Promise of Big Data and Data Science, O'Reilly 2019
* [[practical]] Saurabh Gupta, Practical Enterprise Data Lake Insights: Handle Data-Driven Challenges in an Enterprise Big Data Lake, Apress 2018
* [[bigdata]] Nathan Marz, Big Data, Manning 2019
* [[hadoop]] Tom White, Hadoop: The Definitive Guide, O'Reilly 2009
