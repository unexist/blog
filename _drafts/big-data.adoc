---
layout: post
title: Big Data
date: %%%DATE%%%
last_updated: %%%DATE%%%
author: Christoph Kappel
tags: showcase
categories: showcase hadoop hive iceberg debezium
toc: true
---
:imagesdir: /assets/images/big_data
:figure-caption!:
:table-caption!:

```
https://github.com/unexist/showcase-hadoop-cdc-quarkus/blob/master/podman/hadoop_hive_spark/Dockerfile
https://www.linkedin.com/pulse/musl-libc-alpines-greatest-weakness-rogan-lynch/
https://wiki.musl-libc.org/functional-differences-from-glibc.html
```

I never had much contact with the general topic of [Big Data][] or even [Data Lakes][] before and
I mainly knew both just exists and is a hype.
And recently I had to fill in some blanks in preparation for a meeting for a possible new project.

After reading some books (<<datalake>>, <<practical>> and <<bigdata>>) and blog posts about the
overall idea I was pretty much enticed and wanted to start playing with it.
Setting everything up was a challenge by itself, but the more difficult one was how to get actual
data there in the first place?

So in this blog post we are going to start with a brief introduction of the whole idea, fire up a
single-container data lake and play with two ways of moving data from our usual
[demo application][] into it.

== What is Big Data?

The introduction of big data generally marks the shift away from storing processed or in some ways
refined data to just storing anything - just in case it might become useful later.
Keeping many things in one place has some special requirements for the kind of storage, so the data
is basically just flat files and the step to find a schema is postponed to read.

This got out of hand quickly and the huge piles couldn't be handled anymore, so the big data idea
was revised and broken down into all-purpose data warehouses with a mix of all available domains,
a data lake with a single domain and even finer grained in a data mart.








Handling a huge amount of data comes with problems of its own right.
The first obvious one is it takes a lot of storage space and


One of the major problems of today's data-based systems is the huge complexity, which is required to
address modern expectations of fault-tolerance and obviously the performance or rather latency of
the systems.

This can be easy for smaller scales, but causes lots of headaches in the petabytes range of data.

Unfortunately the storage size is only one-half of the problem, processing large quantities of data
also dramatically increases the processing time up to the point where a timely response cannot be
guaranteed anymore without cutting some corners.

Entering big data!

=== Reducing complexity

The easiest way of reducing complexity is to either avoid it or just blame someone else for it.
And somehow this is the best approach


[link=https://xkcd.com/2582/]
.Source <https://xkcd.com/2582/>
image::data_trap.png[]

== Setting everything up

Although the [Dockerfile][] is quite huge, there isn't much complexity involved, but I still would
like to dedicate some lines to some issues I especially had fun with.

=== Alpine and Musl

The first bigger issues was the incompatibility of the C-based parts of [datanodes][] with the
default [libc][] implementation of [Alpine][].
[Musl][] is an excellent choice due to its small storage footprint for containers, but is not
without problems.

[quote,'https://martinheinz.dev/blog/92']
By using Alpine, you're getting "free" chaos engineering for your cluster.

There are some differences which can to lead to surprising issues and it is never a bad to idea to
check [this curated list][] for more information about it.

The best option for me here was to switch to another base image entirely:

[source,docker]
----
FROM eclipse-temurin:8-jdk
----

=== Podman capabilities

Coming from [ArchLinux][], I obviously still have the **rolling-update-way** deeply ingrained in
me and usually just hit update whenever I see such a message and also fire-up [brew][] and [rustup][]
quite frequently.

This usually works well - until it doesn't.

After updating my local [Podman][] (somehwere around `4.4.1`) installation I discovered I cannot
start [sshd][] inside of my container anymore.
Podman doesn't necessarily need sshd, but [Hadoop][] and friends rely heavily on it for
inter-node-communication:

[source,log]
----
ssh: Connection closed by 127.0.0.1 port 22
sshd: chroot("/run/sshd"): Operation not permitted [preauth]
----

I'd like to say this error directly pointed me to the changelog of Podman, but unfortunately I spent
some hours debugging and desperating until I finally found a hint on [Stack Overflow][] to have a
look at the release notes:

[link=https://github.com/containers/podman/blob/main/RELEASE_NOTES.md]
.Source: https://github.com/containers/podman/blob/main/RELEASE_NOTES.md
image::podman_capabilities.png[]

== Debezium, the WAL and an Iceberg

== Scala with a Spark

[bibliography]
== References

* [[datalake]] Alex Gorelik, The Enterprise Big Data Lake: Delivering the Promise of Big Data and Data Science, O'Reilly 2019
* [[practical]] Saurabh Gupta, Practical Enterprise Data Lake Insights: Handle Data-Driven Challenges in an Enterprise Big Data Lake, Apress 2018
* [[bigdata]] Nathan Marz, Big Data, Manning 2019
