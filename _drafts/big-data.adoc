---
layout: post
title: Big Data
#date: %%%DATE%%%
#last_updated: %%%DATE%%%
author: Christoph Kappel
tags: hadoop big-data showcase
categories: big-data
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/big_data
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/big_data
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

////
https://github.com/unexist/showcase-hadoop-cdc-quarkus/
https://aws.amazon.com/compare/the-difference-between-a-data-warehouse-data-lake-and-data-mart/
////

[Big Data] was just another buzzword for me and I never had any real contact besides the typical
requests of data scientists to move data to a specific endpoint of a [Data Lake] in the overall
architecture.

This changed, when I got the opportunity to have a closer look at a big data project in a business
context.
I usually don't like to deal with new tech stuff unprepared, so I filled in some blanks by reading
articles, a few books (<<datalake>>, <<practical>>, <<bigdata>> and <<hadoop>>) and basically
started playing with it.

This led to lots of insights and Aha!-moments and I really wanted to write about, but unfortunately
not only the name is big.
Condensing all the stuff I'd like to address is quite difficult, so this is the start of a small
series about the groundwork and (at least to me) interesting tools and aspects.

== What is Big Data?

From my impression the biggest thing in the big data idea is the shift away from storing processed
or in some ways refined data to just storing anything as is - just in case it might become useful
later.

Storing everything piles up pretty quickly and gets even worse when you actually work with it:

[link=https://xkcd.com/2582/]
.Source <https://xkcd.com/2582/>
image::data_trap.png[]

=== Reducing complexity

There have been several iterations of tackling the overall complexity on the storage level and like
all good stories it started with SQL.

Once upon a time, there were [Data Warehouses] with big databases, which kept everything in rigid
schemas.
Unfortunately this introduced complex requirements for scaling and made redundancy even worse, but
did the job for a while and everyone was happy...

Later on they were augmented or replaced by [Data Lakes] and [Data Marts], which did some things
differently:

- They contained data split by domain of origin or simply use-case
- No data schema is enforced on write and everything is flat (most likely flat-file)
- They offloaded the problem of handling the actual data to [Hadoop]

== What is Hadoop?

Hadoop is an open-source [Java] framework that manages large quantities of data and provides means
for further processing.
Tasks can be broken down into smaller jobs and are run close to the actual data at the same time.

It consists of four major components:

- *Hadoop Distributed File System (HDFS)* - is the underlying distributed filesystem and allows
the Hadoop nodes to operate on local data.
- *Yet Another Resource Negotiator (YARN)* - is the resource-management part of the system and
responsible for scheduling jobs and resource allocation.
- *MapReduce* - is a programming model for the actual processing in a [divide-and-conquer] way
and is going to be covered in a further post.
- *Hadoop Common* - is a collection of all supporting tools and libraries.

=== Architecture

[link=https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html]
.Source <https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html>
image::hdfsarchitecture.gif[]

=== Files

++++
{% plantuml %}
!theme unexist from ../assets/plantuml
left to right direction

database "Catalog Layer" {
  [Catalog]
}

node "Metadata Layer" {
  [Metadata File]
  [Manifest List]
  [Manifest File]
}


node "Data Layer" {
  [Data Files]
}

[Catalog] --> [Metadata File]
[Metadata File] --> [Manifest List]
[Manifest List] --> [Manifest File]
[Manifest File] --> [Data Files]
{% endplantuml %}
++++


=== Benefits



[bibliography]
== References

* [[[datalake]]] Alex Gorelik, The Enterprise Big Data Lake: Delivering the Promise of Big Data and Data Science, O'Reilly 2019
* [[[practical]]] Saurabh Gupta, Practical Enterprise Data Lake Insights: Handle Data-Driven Challenges in an Enterprise Big Data Lake, Apress 2018
* [[[bigdata]]] Nathan Marz, Big Data, Manning 2019
* [[[hadoop]]] Tom White, Hadoop: The Definitive Guide, O'Reilly 2009
