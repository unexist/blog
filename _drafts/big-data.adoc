---
layout: post
title: Big Data
date: %%%DATE%%%
last_updated: %%%DATE%%%
author: Christoph Kappel
tags: showcase
categories: showcase hadoop hive iceberg debezium
toc: true
---
:imagesdir: /assets/images/big_data
:figure-caption!:
:table-caption!:

```
https://github.com/unexist/showcase-hadoop-cdc-quarkus/blob/master/podman/hadoop_hive_spark/Dockerfile
https://www.linkedin.com/pulse/musl-libc-alpines-greatest-weakness-rogan-lynch/
https://wiki.musl-libc.org/functional-differences-from-glibc.html
```

I never had much contact with the general topic of [Data Lakes][] before, but had to fill in some
blanks in preparation for a meeting for a possible new project.
After reading some books and blog posts about the overall idea I pretty much wanted to start
playing with it.
Setting everything up was a challenge by itself, but the more difficult one was how to get data
there in the first place?

So in this blog post we are going to set up a single-container data lake and play with two ways of
moving data from our usual [demo application][] into it.

== Setting everything up

Although the [Dockerfile][] is quite huge, there isn't much complexity involved and I'd like to
use this paragraph to talk about some fun issues I had with it.

=== Alpine and Musl

The first bigger issues that I faced was the incompatibility of the C-based parts of [datanodes][]
with the default [libc][] implementation of [Alpine][].
[Musl][] is an excellent choice due to its small storage footprint, which happens to be one of the
concerns with containers, but is not without problems.
There are some differences which can to lead to surprising issues and it is never a bad to idea to
check [this curated list][] for more information about it.

The best option for me here was to switch to another base image.

[quote,'https://martinheinz.dev/blog/92']
By using Alpine, you're getting "free" chaos engineering for your cluster.

=== Podman capabilities

One of the drawbacks of using software in a **rolling-update-way** is that you run head first into
every breaking change, unless you make it a good habit of checking changelogs before doing the
actual update.
Needless to say I didn't do that and discovered a nice change introduced in [Podman][] `4.4.1`.

After updating my local Podman installation I couldn't start [sshd][] inside of my container
anymore and [Hadoop][] and friends rely heavily on it for communication between nodes:

[source,log]
----
ssh: Connection closed by 127.0.0.1 port 22
sshd: chroot("/run/sshd"): Operation not permitted [preauth]
----

I'd like to say seeing this error pointed me directly to the changelog of Podman, but unfortunately
I had to debug some time and cannot really remember
I cannot exactly tell how I

[link=https://github.com/containers/podman/blob/main/RELEASE_NOTES.md]
.Source: https://github.com/containers/podman/blob/main/RELEASE_NOTES.md
image::podman_capabilities.png[]

== Debezium, the WAL and an Iceberg

== Scala with a Spark