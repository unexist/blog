---
layout: post
title: Monitoring with SigNoz
description: TBD
#date: %%%DATE%%%
#last_updated: %%%DATE%%%
author: Christoph Kappel
tags: tracing opentelemetry logging prometheus logsprout alertmanager signozshowcase
categories: observability
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/monitoring_with_signoz
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/monitoring_with_signoz
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

Finding good reasoning to explore different options for monitoring or better [observability] is difficult.
Either there wasn't the singular impact on production yet, that made you lust for better monitoring and/or it is
difficult to understand the merit and invest of time.

And even when you make the decision to dive into it, a good advise is probably not to start on production, but with
a simple example.
Simple examples on the other hand rarely show the real powers, so it usually ends in heavily contrived ones like
the one I've used in my last blogpost about
{{ site.url }}{% post_url 2022-02-18-logging-vs-tracing %}[Logging vs Tracing].

Still, nobody got fired from [line-through]#buying IBM# ramping up monitoring, so let us - for the course of this
post - put our [EFK][] stack and friends aside and get started with something shiny new.

== What is SigNoz?

If you are like me and you haven't heard the name [SigNoz][] before the first and foremost questions are probably
what is SigNoz and why not one of these solutions `insert random product here`.

From a marketing perspective the key selling point for me probably and honestly was the headline on the
frontpage:

[quote,https://signoz.io]
____
OpenTelemetry-Native Logs, Metrics and Traces in a single pane
____

Without knowing prior to that, this was exactly what I need, so well done marketing:

. [FOSS][]
. Single solution to address the [three pillars][]
. Nice and complete package

That sounds rather too good, but time to put my wizard hat and to check the brief.
Before even messing with [docker-compose][] they thankfully provide an architecture overview and this looks like
they hold their part of the deal:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 20
skinparam ranksep 20

rectangle "App (1))" as app #crimson
rectangle "Otel Collector (2)" as otel #green

card "SigNoz" as signoz {
    rectangle "SigNoz Otel Collector (3)" as sigotel
    database "Clickhouse (4)" as clickhouse
    rectangle "Query Service (5)" as query
    rectangle "Alert Manager (6)" as alert
    rectangle "Frontend (7)" as fe
}

app -[hidden]d-> otel

app -r-> sigotel
otel -l-> sigotel

sigotel --> clickhouse
clickhouse --> query
query <--> alert
query <-> fe
{% endplantuml %}
++++
<1> Apps can directly send data to SigNoz
<2> [Otel collectors][] can transmit data as well
<3> Internally another collector provides the endpoints to receive data
<4> Columnar storage? Though I haven't heared of [Clickhouse][] either before
<5> Some abstraction to query the actual data
<6> [Alert Manager][] keeps tracks and handles all the various alerts - glad they haven't reinvented the wheel
<7> And the shiny bit we've spoken of before

== Collecting data

Once Signoz is running, which basically boils down to calling docker-compose, a nice trick question is how to
deliver your actual data to it.

[Opentelemetry][] is the defacto standard for that and offers many ways to gather, collect and transmit
data via highly configurable [pipelines][].
The only noteworthy thing here to pay attention to the size of the generated logs file - which may cause some
headaches as it did for me.

While playing with SigNoz I discovered it doesn't connect each of its containers separately to an
[Opentelemetry Collector][].footnote:[otelcol in short], but passes this task to [logspout][].

We've already know marketing works for me, so try to guess what their words did to me:

[quote,https://github.com/gliderlabs/logspout]
____
Logspout is a log router for Docker containers that runs inside Docker. It attaches to all containers on a host,
then routes their logs wherever you want. It also has an extensible module system.
____

Right, this sounded like a splendid idea and is exactly what I do inside of my example.
In fact, there isn't much we have to do at all config-wise:

. [Docker][] needs a minimal config to get us started:
+
[source,yaml]
----
  logspout:
    container_name: todo-logspout
    image: "docker.io/gliderlabs/logspout:latest"
    pull_policy: if_not_present
    volumes:
      - /etc/hostname:/etc/host_hostname:ro
      - /var/run/docker.sock:/var/run/docker.sock
    command: syslog+tcp://otelcol:2255 # (1)
    depends_on:
      - otelcol
    restart: on-failure
----
<1> This configures a connection to a receiver of our otelcol instance and comes up next

. And we have to define a receiver in otelcol:
+
[source,yaml]
----
receivers:
  tcplog/docker:
    listen_address: "0.0.0.0:2255"
    operators:
      - type: regex_parser
        regex: '^<([0-9]+)>[0-9]+ (?P<timestamp>[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}(\.[0-9]+)?([zZ]|([\+-])([01]\d|2[0-3]):?([0-5]\d)?)?) (?P<container_id>\S+) (?P<container_name>\S+) [0-9]+ - -( (?P<body>.*))?'
        timestamp:
          parse_from: attributes.timestamp
          layout: '%Y-%m-%dT%H:%M:%S.%LZ'
      - type: move
        from: attributes["body"]
        to: body
      - type: remove
        field: attributes.timestamp
      - type: filter
        id: logs_filter
        expr: 'attributes.container_name matches "^todo-(postgres|otelcol|logsput)"'
      - type: json_parser
        parse_form: body
----


== Pillars in practice

There is plenty of explanation and definition out there, way better than I can ever provide, but just to
call the three back to your memory:

[cols="1,5"]
|===
| Logging
| Historical records of system events and errors

| Tracing
| Visualization of requests flowing through (distributed) systems

| Metrics
| Numerical data like e.g. performance, response time, memory consumption
|===

=== Logging

My previous article explain in detail why [structured logging][] in general is a good idea, so we can just reap
the fruits and throw in [zerolog][] in a [Gin-gonic middleware][]:

[source,go]
----
logEvent.Str("client_id", param.ClientIP).
    Str("method", param.Method).
    Int("status_code", param.StatusCode).
    Int("body_size", param.BodySize).
    Str("path", param.Path).
    Str("latency", param.Latency.String()).
    Msg(param.ErrorMessage)
----

SigNoz offers lots of different options to search data and if ever tried to use [Kibana][] and the likes you
propaly feel at home right away:

image::logs.png[]

There is also no reason to shy away if you require some kind of aggregation and diagrams with bars:

image::logs-aggregate.png[]

=== Tracing

image::traces.png[]

=== Metrics

image::metrics.png[]

=== Alerts

image::alerts.png[]

== Conclusion

All examples can be found here:

<https://github.com/unexist/showcase-microservices-golang>
