---
layout: post
title: Monitoring with SigNoz
description: TBD
#date: %%%DATE%%%
#last_updated: %%%DATE%%%
author: Christoph Kappel
tags: tracing opentelemetry logging prometheus logsprout alertmanager signozshowcase
categories: observability
toc: true
---
ifdef::asciidoctorconfigdir[]
:imagesdir: {asciidoctorconfigdir}/../assets/images/monitoring_with_signoz
endif::[]
ifndef::asciidoctorconfigdir[]
:imagesdir: /assets/images/monitoring_with_signoz
endif::[]
:figure-caption!:
:table-caption!:
:page-liquid:

Finding good reasoning to explore different options for monitoring or better [observability] is difficult.
Either there wasn't the singular impact on production yet, that made you lust for better monitoring and/or it is
difficult to understand the merit and invest of time.

And even when you make the decision to dive into it, it is always a good idea not to start on production, but with
a simple example.
Simple examples on the other hand rarely show the real powers, so it usually ends in heavily contrived ones like
the one I've used in my last post about
{{ site.url }}{% post_url 2022-02-18-logging-vs-tracing %}[Logging vs Tracing].

Still, [nobody got fired from [line-through]#buying IBM#][] ramping up monitoring, so let us - for the course of this
post - put our [EFK][] stack and friends aside and get started with something shiny new.

== What is SigNoz?

If you are like me and you haven't heard the name [SigNoz][] before the first and foremost questions are probably
what is SigNoz and why not one of these solutions `insert random product here`.

From a marketing perspective the key selling point for me probably and honestly was the headline on the
frontpage:

[quote,https://signoz.io]
____
OpenTelemetry-Native Logs, Metrics and Traces in a single pane
____

Without knowing prior to that, this was exactly what I need, so well done marketing:

. Seems to be [FOSS][]
. Single solution to address the [three pillars][]
. Nice and complete package

That sounds rather too good, but time to put on my wizard hat and to check the brief.
Before messing with [docker-compose][], I checked the documentation and discovered an architecture overview and
this looks like they hold their part of the bargain:

++++
{% plantuml %}
!theme unexist from {{ site.asciidoctor_attributes.plantumldir }}

skinparam linetype ortho
skinparam nodesep 20
skinparam ranksep 20

rectangle "App (1)" as app #crimson
rectangle "Otel Collector (2)" as otel #green

card "SigNoz" as signoz {
    rectangle "SigNoz Otel Collector (3)" as sigotel
    database "Clickhouse (4)" as clickhouse
    rectangle "Query Service (5)" as query
    rectangle "Alert Manager (6)" as alert
    rectangle "Frontend (7)" as fe
}

app -[hidden]d-> otel

app -r-> sigotel
otel -l-> sigotel

sigotel --> clickhouse
clickhouse --> query
query <--> alert
query <-> fe
{% endplantuml %}
++++
<1> Apps can directly send data to SigNoz
<2> [Otel collectors][] can transmit data as well
<3> Internally another custom collector provides the endpoints to receive data
<4> Though I haven't heared of [Clickhouse][] either before, but columnar storage sounds about right
<5> Some abstraction to query the actual data
<6> [Alert Manager][] keeps tracks and handles all the various alerts - glad they haven't reinvented the wheel
<7> And the shiny bit we've spoken of before

== Collecting data

Once Signoz is running, which basically boils down to calling docker-compose, a nice starter question is how to
deliver your actual data to it.

[Opentelemetry][] is the defacto standard for that and offers many ways to gather, collect and transmit
data via highly configurable [pipelines][].
The only noteworthy thing here to pay attention to the size of the generated logs - which may cause some
headaches as it did for me during my vacation.

While playing with SigNoz I discovered it doesn't connect each of its containers separately to an
[Opentelemetry Collector][].footnote:[otelcol in short], but passes this task entirely to a container
with [logspout][].

After a quick glance at the [Github][] page marketing did its thing again:

[quote,https://github.com/gliderlabs/logspout]
____
Logspout is a log router for Docker containers that runs inside Docker. It attaches to all containers on a host,
then routes their logs wherever you want. It also has an extensible module system.
____

Right, this sounded like a splendid idea and is exactly what I do in my example.
In fact, there isn't much we have to configure at all:

. [Docker][] needs a minimal config to get us started:
+
[source,yaml]
----
  logspout:
    container_name: todo-logspout
    image: "docker.io/gliderlabs/logspout:latest"
    pull_policy: if_not_present
    volumes:
      - /etc/hostname:/etc/host_hostname:ro
      - /var/run/docker.sock:/var/run/docker.sock # <2>
    command: syslog+tcp://otelcol:2255 # <1>
    depends_on:
      - otelcol
    restart: on-failure
----
<1> Logspout needs access to Dockler
<2> This configures a connection to a receiver of our otelcol instance and comes up next

. And we have to define a receiver in otelcol:
+
[source,yaml]
----
receivers:
  tcplog/docker:
    listen_address: "0.0.0.0:2255"
    operators: # <1>
      - type: regex_parser # <2>
        regex: '^<([0-9]+)>[0-9]+ (?P<timestamp>[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}(\.[0-9]+)?([zZ]|([\+-])([01]\d|2[0-3]):?([0-5]\d)?)?) (?P<container_id>\S+) (?P<container_name>\S+) [0-9]+ - -( (?P<body>.*))?'
        timestamp:
          parse_from: attributes.timestamp
          layout: '%Y-%m-%dT%H:%M:%S.%LZ'
      - type: move # <3>
        from: attributes["body"]
        to: body
      - type: remove
        field: attributes.timestamp
      - type: filter # <4>
        id: logs_filter
        expr: 'attributes.container_name matches "^todo-(postgres|otelcol|logspout)"'
      - type: json_parser
        parse_form: body
----
<1> Operators allow to parse, modify and filter entries
<2> This is the default format of the messages logspout forwards to otelcol
<3> We basically move our content to the actual body of the entry
<4> There might be lots of different containers running, so we limit the entries based on container names

== Pillars in practice

There is plenty of explanation and definition out there, way better than I can ever provide, but just to
recall the three back to our memory:

[cols="1,5"]
|===
| Logging
| Historical records of system events and errors

| Tracing
| Visualization of requests flowing through (distributed) systems

| Metrics
| Numerical data like e.g. performance, response time, memory consumption
|===

=== Logging

{{ site.url }}{% post_url 2022-02-18-logging-vs-tracing %}#structured-logs[Structured logging] is in general a really
good idea, so we throw in [zerolog][] and do all the handling in a simple [Gin-gonic middleware][]:

[source,go]
----
logEvent.Str("client_id", param.ClientIP). // <1>
    Str("method", param.Method).
    Int("status_code", param.StatusCode).
    Int("body_size", param.BodySize).
    Str("path", param.Path).
    Str("latency", param.Latency.String()).
    Msg(param.ErrorMessage)
----
<1> All the mapping and magic happens here

SigNoz offers lots of different options to search data and if ever tried to use [Kibana][] and the likes you
propaly feel at home right away:

image::logs.png[]

There is also no reason to shy away if you require some kind of aggregation and diagrams with bars:

image::logs-aggregate.png[]

=== Tracing

image::traces.png[]

=== Metrics

image::metrics.png[]

=== Alerts

image::alerts.png[]

== Conclusion

All examples can be found here:

<https://github.com/unexist/showcase-microservices-golang>
